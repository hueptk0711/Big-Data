{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/groda/big_data/blob/master/MapReduce_Primer_HelloWorld.ipynb","timestamp":1713893996691}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["<a href=\"https://github.com/groda/big_data\"><div><img src=\"https://github.com/groda/big_data/blob/master/logo_bdb.png?raw=true\" align=right width=\"90\"></div></a>\n","\n","# MapReduce: A Primer with <code>Hello World!</code>\n","<br>\n","<br>\n","\n","For this tutorial, we are going to download the core Hadoop distribution and run Hadoop in _local standalone mode_:\n","\n","> ❝ _By default, Hadoop is configured to run in a non-distributed mode, as a single Java process._ ❞\n","\n","(see [https://hadoop.apache.org/docs/stable/.../Standalone_Operation](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Standalone_Operation))\n","\n","We are going to run a MapReduce job using MapReduce's [streaming application](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Hadoop_Streaming). This is not to be confused with real-time streaming:\n","\n","> ❝ _Hadoop streaming is a utility that comes with the Hadoop distribution. The utility allows you to create and run Map/Reduce jobs with any executable or script as the mapper and/or the reducer._ ❞\n","\n","MapReduce streaming defaults to using [`IdentityMapper`](https://hadoop.apache.org/docs/stable/api/index.html) and [`IdentityReducer`](https://hadoop.apache.org/docs/stable/api/index.html), thus eliminating the need for explicit specification of a mapper or reducer. Finally, we show how to run a map-only job by setting `mapreduce.job.reduce` equal to $0$.\n","\n","Both input and output are standard files since Hadoop's default filesystem is the regular file system, as specified by the `fs.defaultFS` property in [core-default.xml](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/core-default.xml)).\n"],"metadata":{"id":"GzbmlR27wh6e"}},{"cell_type":"markdown","source":["# Download core Hadoop"],"metadata":{"id":"uUbM5R0GwwYw"}},{"cell_type":"code","source":["HADOOP_URL = \"https://dlcdn.apache.org/hadoop/common/stable/hadoop-3.4.0.tar.gz\"\n","\n","import requests\n","import os\n","import tarfile\n","\n","def download_and_extract_targz(url):\n","    response = requests.get(url)\n","    filename = url.rsplit('/', 1)[-1]\n","    HADOOP_HOME = filename[:-7]\n","    # set HADOOP_HOME environment variable\n","    os.environ['HADOOP_HOME'] = HADOOP_HOME\n","    if os.path.isdir(HADOOP_HOME):\n","      print(\"Not downloading, Hadoop folder {} already exists\".format(HADOOP_HOME))\n","      return\n","    if response.status_code == 200:\n","        with open(filename, 'wb') as file:\n","            file.write(response.content)\n","        with tarfile.open(filename, 'r:gz') as tar_ref:\n","            extract_path = tar_ref.extractall(path='.')\n","            # Get the names of all members (files and directories) in the archive\n","            all_members = tar_ref.getnames()\n","            # If there is a top-level directory, get its name\n","            if all_members:\n","              top_level_directory = all_members[0]\n","              print(f\"ZIP file downloaded and extracted successfully. Contents saved at: {top_level_directory}\")\n","    else:\n","        print(f\"Failed to download ZIP file. Status code: {response.status_code}\")\n","\n","\n","download_and_extract_targz(HADOOP_URL)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jDgQtQlzw8bL","outputId":"3b35a438-f7bc-4519-b465-5c54191a9a09","executionInfo":{"status":"ok","timestamp":1713893744381,"user_tz":-420,"elapsed":31530,"user":{"displayName":"","userId":""}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["ZIP file downloaded and extracted successfully. Contents saved at: hadoop-3.4.0\n"]}]},{"cell_type":"markdown","source":["# Set environment variables"],"metadata":{"id":"3yvb5cw9xEbh"}},{"cell_type":"markdown","source":["## Set `HADOOP_HOME` and `PATH`"],"metadata":{"id":"u6lkrz1dxIiO"}},{"cell_type":"code","source":["# HADOOP_HOME was set earlier when downloading Hadoop distribution\n","print(\"HADOOP_HOME is {}\".format(os.environ['HADOOP_HOME']))\n","\n","os.environ['PATH'] = ':'.join([os.path.join(os.environ['HADOOP_HOME'], 'bin'), os.environ['PATH']])\n","print(\"PATH is {}\".format(os.environ['PATH']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s7maAwaFxBT_","outputId":"ea824197-8f48-4477-d429-a6862771f7fe","executionInfo":{"status":"ok","timestamp":1713893750407,"user_tz":-420,"elapsed":650,"user":{"displayName":"","userId":""}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["HADOOP_HOME is hadoop-3.4.0\n","PATH is hadoop-3.4.0/bin:/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"]}]},{"cell_type":"markdown","source":["## Set `JAVA_HOME`\n","\n","While Java is readily available on Google Colab, we consider the broader scenario of an Ubuntu machine. In this case, we ensure compatibility by installing Java, specifically opting for the `openjdk-19-jre-headless` version."],"metadata":{"id":"4kzJ8cNoxPyK"}},{"cell_type":"code","source":["import shutil\n","\n","# set variable JAVA_HOME (install Java if necessary)\n","def is_java_installed():\n","    os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n","    return os.environ['JAVA_HOME']\n","\n","def install_java():\n","    # Uncomment and modify the desired version\n","    # java_version= 'openjdk-11-jre-headless'\n","    # java_version= 'default-jre'\n","    # java_version= 'openjdk-17-jre-headless'\n","    # java_version= 'openjdk-18-jre-headless'\n","    java_version= 'openjdk-19-jre-headless'\n","\n","    print(f\"Java not found. Installing {java_version} ... (this might take a while)\")\n","    try:\n","        cmd = f\"apt install -y {java_version}\"\n","        subprocess_output = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n","        stdout_result = subprocess_output.stdout\n","        # Process the results as needed\n","        print(\"Done installing Java {}\".format(java_version))\n","        os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n","        print(\"JAVA_HOME is {}\".format(os.environ['JAVA_HOME']))\n","    except subprocess.CalledProcessError as e:\n","        # Handle the error if the command returns a non-zero exit code\n","        print(\"Command failed with return code {}\".format(e.returncode))\n","        print(\"stdout: {}\".format(e.stdout))\n","\n","# Install Java if not available\n","if is_java_installed():\n","    print(\"Java is already installed: {}\".format(os.environ['JAVA_HOME']))\n","else:\n","    print(\"Installing Java\")\n","    install_java()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SauFHVPOxL-Y","outputId":"17fd4209-973c-4b9b-9d2c-d678c9e164e0","executionInfo":{"status":"ok","timestamp":1713893754159,"user_tz":-420,"elapsed":441,"user":{"displayName":"","userId":""}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Java is already installed: /usr/lib/jvm/java-11-openjdk-amd64\n"]}]},{"cell_type":"markdown","source":["# Run a MapReduce job with Hadoop streaming"],"metadata":{"id":"6HFPVX84xbNd"}},{"cell_type":"markdown","source":["## Create a file\n","\n","Write the string\"Hello, World!\" to a local file.<p>**Note:** you will be writing to the file `./hello.txt` in your current directory (denoted by `./`)."],"metadata":{"id":"_yVa55X1xmOb"}},{"cell_type":"code","source":["!echo \"Hello, World!\">./hello.txt"],"metadata":{"id":"9Jz7mJkcxYxw","executionInfo":{"status":"ok","timestamp":1713893759809,"user_tz":-420,"elapsed":336,"user":{"displayName":"","userId":""}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["## Launch the MapReduce \"Hello, World!\" application\n","\n","Since the default filesystem is the local filesystem (as opposed to HDFS) we do not need to upload the local file `hello.txt` to HDFS.\n","\n","Run a MapReduce job with `/bin/cat` as a mapper and no reducer.\n","\n","**Note:** the first step of removing the output directory is necessary because MapReduce does not overwrite data folders by design."],"metadata":{"id":"zSh_Kr5Bxvst"}},{"cell_type":"code","source":["%%bash\n","hdfs dfs -rm -r my_output\n","\n","mapred streaming \\\n","    -input hello.txt \\\n","    -output my_output \\\n","    -mapper '/bin/cat'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nb5JryK9xpPA","outputId":"1556159b-3831-4964-9f2d-7085b3752f30","executionInfo":{"status":"ok","timestamp":1713893770397,"user_tz":-420,"elapsed":7236,"user":{"displayName":"","userId":""}}},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["rm: `my_output': No such file or directory\n","2024-04-23 17:36:08,913 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n","2024-04-23 17:36:09,168 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n","2024-04-23 17:36:09,168 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n","2024-04-23 17:36:09,193 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2024-04-23 17:36:09,528 INFO mapred.FileInputFormat: Total input files to process : 1\n","2024-04-23 17:36:09,565 INFO mapreduce.JobSubmitter: number of splits:1\n","2024-04-23 17:36:10,061 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1088611167_0001\n","2024-04-23 17:36:10,061 INFO mapreduce.JobSubmitter: Executing with tokens: []\n","2024-04-23 17:36:10,325 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n","2024-04-23 17:36:10,327 INFO mapreduce.Job: Running job: job_local1088611167_0001\n","2024-04-23 17:36:10,339 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n","2024-04-23 17:36:10,343 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n","2024-04-23 17:36:10,350 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-04-23 17:36:10,351 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-04-23 17:36:10,431 INFO mapred.LocalJobRunner: Waiting for map tasks\n","2024-04-23 17:36:10,438 INFO mapred.LocalJobRunner: Starting task: attempt_local1088611167_0001_m_000000_0\n","2024-04-23 17:36:10,494 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-04-23 17:36:10,495 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-04-23 17:36:10,561 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2024-04-23 17:36:10,586 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n","2024-04-23 17:36:10,607 INFO mapred.MapTask: numReduceTasks: 1\n","2024-04-23 17:36:10,718 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2024-04-23 17:36:10,718 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2024-04-23 17:36:10,718 INFO mapred.MapTask: soft limit at 83886080\n","2024-04-23 17:36:10,718 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2024-04-23 17:36:10,718 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2024-04-23 17:36:10,742 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2024-04-23 17:36:10,750 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n","2024-04-23 17:36:10,764 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n","2024-04-23 17:36:10,771 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n","2024-04-23 17:36:10,771 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n","2024-04-23 17:36:10,772 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n","2024-04-23 17:36:10,773 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n","2024-04-23 17:36:10,773 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n","2024-04-23 17:36:10,775 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n","2024-04-23 17:36:10,777 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n","2024-04-23 17:36:10,778 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n","2024-04-23 17:36:10,778 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n","2024-04-23 17:36:10,779 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n","2024-04-23 17:36:10,780 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n","2024-04-23 17:36:10,860 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2024-04-23 17:36:10,863 INFO streaming.PipeMapRed: Records R/W=1/1\n","2024-04-23 17:36:10,863 INFO streaming.PipeMapRed: MRErrorThread done\n","2024-04-23 17:36:10,864 INFO streaming.PipeMapRed: mapRedFinished\n","2024-04-23 17:36:10,868 INFO mapred.LocalJobRunner: \n","2024-04-23 17:36:10,868 INFO mapred.MapTask: Starting flush of map output\n","2024-04-23 17:36:10,868 INFO mapred.MapTask: Spilling map output\n","2024-04-23 17:36:10,868 INFO mapred.MapTask: bufstart = 0; bufend = 15; bufvoid = 104857600\n","2024-04-23 17:36:10,868 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n","2024-04-23 17:36:10,887 INFO mapred.MapTask: Finished spill 0\n","2024-04-23 17:36:10,910 INFO mapred.Task: Task:attempt_local1088611167_0001_m_000000_0 is done. And is in the process of committing\n","2024-04-23 17:36:10,914 INFO mapred.LocalJobRunner: Records R/W=1/1\n","2024-04-23 17:36:10,915 INFO mapred.Task: Task 'attempt_local1088611167_0001_m_000000_0' done.\n","2024-04-23 17:36:10,929 INFO mapred.Task: Final Counters for attempt_local1088611167_0001_m_000000_0: Counters: 17\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=141914\n","\t\tFILE: Number of bytes written=857635\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1\n","\t\tMap output records=1\n","\t\tMap output bytes=15\n","\t\tMap output materialized bytes=23\n","\t\tInput split bytes=75\n","\t\tCombine input records=0\n","\t\tSpilled Records=1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=26\n","\t\tTotal committed heap usage (bytes)=378535936\n","\tFile Input Format Counters \n","\t\tBytes Read=14\n","2024-04-23 17:36:10,929 INFO mapred.LocalJobRunner: Finishing task: attempt_local1088611167_0001_m_000000_0\n","2024-04-23 17:36:10,930 INFO mapred.LocalJobRunner: map task executor complete.\n","2024-04-23 17:36:10,934 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n","2024-04-23 17:36:10,945 INFO mapred.LocalJobRunner: Starting task: attempt_local1088611167_0001_r_000000_0\n","2024-04-23 17:36:10,971 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-04-23 17:36:10,971 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-04-23 17:36:10,972 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2024-04-23 17:36:10,978 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@49c1c039\n","2024-04-23 17:36:10,992 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2024-04-23 17:36:11,041 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n","2024-04-23 17:36:11,061 INFO reduce.EventFetcher: attempt_local1088611167_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n","2024-04-23 17:36:11,154 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1088611167_0001_m_000000_0 decomp: 19 len: 23 to MEMORY\n","2024-04-23 17:36:11,169 INFO reduce.InMemoryMapOutput: Read 19 bytes from map-output for attempt_local1088611167_0001_m_000000_0\n","2024-04-23 17:36:11,176 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 19, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->19\n","2024-04-23 17:36:11,181 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n","2024-04-23 17:36:11,183 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-04-23 17:36:11,183 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n","2024-04-23 17:36:11,200 INFO mapred.Merger: Merging 1 sorted segments\n","2024-04-23 17:36:11,208 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n","2024-04-23 17:36:11,210 INFO reduce.MergeManagerImpl: Merged 1 segments, 19 bytes to disk to satisfy reduce memory limit\n","2024-04-23 17:36:11,211 INFO reduce.MergeManagerImpl: Merging 1 files, 23 bytes from disk\n","2024-04-23 17:36:11,212 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n","2024-04-23 17:36:11,212 INFO mapred.Merger: Merging 1 sorted segments\n","2024-04-23 17:36:11,214 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n","2024-04-23 17:36:11,215 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-04-23 17:36:11,237 INFO mapred.Task: Task:attempt_local1088611167_0001_r_000000_0 is done. And is in the process of committing\n","2024-04-23 17:36:11,243 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-04-23 17:36:11,243 INFO mapred.Task: Task attempt_local1088611167_0001_r_000000_0 is allowed to commit now\n","2024-04-23 17:36:11,249 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1088611167_0001_r_000000_0' to file:/content/my_output\n","2024-04-23 17:36:11,253 INFO mapred.LocalJobRunner: reduce > reduce\n","2024-04-23 17:36:11,260 INFO mapred.Task: Task 'attempt_local1088611167_0001_r_000000_0' done.\n","2024-04-23 17:36:11,262 INFO mapred.Task: Final Counters for attempt_local1088611167_0001_r_000000_0: Counters: 24\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=141992\n","\t\tFILE: Number of bytes written=857685\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=1\n","\t\tReduce shuffle bytes=23\n","\t\tReduce input records=1\n","\t\tReduce output records=1\n","\t\tSpilled Records=1\n","\t\tShuffled Maps =1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=1\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=378535936\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Output Format Counters \n","\t\tBytes Written=27\n","2024-04-23 17:36:11,263 INFO mapred.LocalJobRunner: Finishing task: attempt_local1088611167_0001_r_000000_0\n","2024-04-23 17:36:11,263 INFO mapred.LocalJobRunner: reduce task executor complete.\n","2024-04-23 17:36:11,334 INFO mapreduce.Job: Job job_local1088611167_0001 running in uber mode : false\n","2024-04-23 17:36:11,335 INFO mapreduce.Job:  map 100% reduce 100%\n","2024-04-23 17:36:11,337 INFO mapreduce.Job: Job job_local1088611167_0001 completed successfully\n","2024-04-23 17:36:11,375 INFO mapreduce.Job: Counters: 30\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=283906\n","\t\tFILE: Number of bytes written=1715320\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1\n","\t\tMap output records=1\n","\t\tMap output bytes=15\n","\t\tMap output materialized bytes=23\n","\t\tInput split bytes=75\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=1\n","\t\tReduce shuffle bytes=23\n","\t\tReduce input records=1\n","\t\tReduce output records=1\n","\t\tSpilled Records=2\n","\t\tShuffled Maps =1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=1\n","\t\tGC time elapsed (ms)=26\n","\t\tTotal committed heap usage (bytes)=757071872\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Input Format Counters \n","\t\tBytes Read=14\n","\tFile Output Format Counters \n","\t\tBytes Written=27\n","2024-04-23 17:36:11,375 INFO streaming.StreamJob: Output directory: my_output\n"]}]},{"cell_type":"markdown","source":["## Verify the result\n","\n","If the job executed successfully, an empty file named `_SUCCESS` is expected to be present in the output directory `my_output`.\n","\n","Verify the success of the MapReduce job by checking for the presence of the `_SUCCESS` file."],"metadata":{"id":"OB_fX9u5x55y"}},{"cell_type":"code","source":["%%bash\n","\n","echo \"Check if MapReduce job was successful\"\n","hdfs dfs -test -e my_output/_SUCCESS\n","if [ $? -eq 0 ]; then\n","\techo \"_SUCCESS exists!\"\n","fi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bnvEvYDfx2g4","outputId":"c50c139b-b906-4b8e-e58f-842087042db6","executionInfo":{"status":"ok","timestamp":1713893778252,"user_tz":-420,"elapsed":2671,"user":{"displayName":"","userId":""}}},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Check if MapReduce job was successful\n","_SUCCESS exists!\n"]}]},{"cell_type":"markdown","source":["**Note:** `hdfs dfs -ls` is the same as `ls` since the default filesystem is the local filesystem."],"metadata":{"id":"BLMnBh44x_YR"}},{"cell_type":"code","source":["!hdfs dfs -ls my_output"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ufAfmGUvx8jW","outputId":"18e55734-133d-4cb6-fbcb-560bd68914c3","executionInfo":{"status":"ok","timestamp":1713893784937,"user_tz":-420,"elapsed":3199,"user":{"displayName":"","userId":""}}},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 2 items\n","-rw-r--r--   1 root root          0 2024-04-23 17:36 my_output/_SUCCESS\n","-rw-r--r--   1 root root         15 2024-04-23 17:36 my_output/part-00000\n"]}]},{"cell_type":"code","source":["!ls -l my_output"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZnKSahPzyCAn","outputId":"d1edce2c-8f18-4b0f-cf68-fec962cec873","executionInfo":{"status":"ok","timestamp":1713893787612,"user_tz":-420,"elapsed":358,"user":{"displayName":"","userId":""}}},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["total 4\n","-rw-r--r-- 1 root root 15 Apr 23 17:36 part-00000\n","-rw-r--r-- 1 root root  0 Apr 23 17:36 _SUCCESS\n"]}]},{"cell_type":"markdown","source":["The actual output of the MapReduce job is contained in the file `part-00000` in the output directory."],"metadata":{"id":"v9LmpcaMyG23"}},{"cell_type":"code","source":["!cat my_output/part-00000"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eL-Clat5yD8I","outputId":"edeca7f8-d159-415f-dc24-c74c156d7377","executionInfo":{"status":"ok","timestamp":1713893790337,"user_tz":-420,"elapsed":439,"user":{"displayName":"","userId":""}}},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Hello, World!\t\n"]}]},{"cell_type":"markdown","source":["# MapReduce without specifying mapper or reducer\n","\n","In the previous example, we have seen how to run a MapReduce job without specifying any reducer.\n","\n","Since the only required options for `mapred streaming` are `input` and `output`, we can also run a MapReduce job without specifying a mapper."],"metadata":{"id":"AmpHr_HyyMnM"}},{"cell_type":"code","source":["!mapred streaming -h"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZPWL1AiXyJac","outputId":"8b0299d1-0815-4e70-fec9-30722af51d46","executionInfo":{"status":"ok","timestamp":1713893795315,"user_tz":-420,"elapsed":2383,"user":{"displayName":"","userId":""}}},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-04-23 17:36:36,257 ERROR streaming.StreamJob: Unrecognized option: -h\n","Usage: $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar [options]\n","Options:\n","  -input          <path> DFS input file(s) for the Map step.\n","  -output         <path> DFS output directory for the Reduce step.\n","  -mapper         <cmd|JavaClassName> Optional. Command to be run as mapper.\n","  -combiner       <cmd|JavaClassName> Optional. Command to be run as combiner.\n","  -reducer        <cmd|JavaClassName> Optional. Command to be run as reducer.\n","  -file           <file> Optional. File/dir to be shipped in the Job jar file.\n","                  Deprecated. Use generic option \"-files\" instead.\n","  -inputformat    <TextInputFormat(default)|SequenceFileAsTextInputFormat|JavaClassName>\n","                  Optional. The input format class.\n","  -outputformat   <TextOutputFormat(default)|JavaClassName>\n","                  Optional. The output format class.\n","  -partitioner    <JavaClassName>  Optional. The partitioner class.\n","  -numReduceTasks <num> Optional. Number of reduce tasks.\n","  -inputreader    <spec> Optional. Input recordreader spec.\n","  -cmdenv         <n>=<v> Optional. Pass env.var to streaming commands.\n","  -mapdebug       <cmd> Optional. To run this script when a map task fails.\n","  -reducedebug    <cmd> Optional. To run this script when a reduce task fails.\n","  -io             <identifier> Optional. Format to use for input to and output\n","                  from mapper/reducer commands\n","  -lazyOutput     Optional. Lazily create Output.\n","  -background     Optional. Submit the job and don't wait till it completes.\n","  -verbose        Optional. Print verbose output.\n","  -info           Optional. Print detailed usage.\n","  -help           Optional. Print help message.\n","\n","Generic options supported are:\n","-conf <configuration file>        specify an application configuration file\n","-D <property=value>               define a value for a given property\n","-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\n","-jt <local|resourcemanager:port>  specify a ResourceManager\n","-files <file1,...>                specify a comma-separated list of files to be copied to the map reduce cluster\n","-libjars <jar1,...>               specify a comma-separated list of jar files to be included in the classpath\n","-archives <archive1,...>          specify a comma-separated list of archives to be unarchived on the compute machines\n","\n","The general command line syntax is:\n","command [genericOptions] [commandOptions]\n","\n","\n","For more details about these options:\n","Use $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar -info\n","\n","Try -help for more information\n","Streaming Command Failed!\n"]}]},{"cell_type":"code","source":["%%bash\n","hdfs dfs -rm -r my_output\n","\n","mapred streaming \\\n","    -input hello.txt \\\n","    -output my_output"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5H2MkIUPyQc2","outputId":"ac9a35cb-3bb9-43c9-cb7e-08ac15fdb746","executionInfo":{"status":"ok","timestamp":1713893809103,"user_tz":-420,"elapsed":7828,"user":{"displayName":"","userId":""}}},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Deleted my_output\n"]},{"output_type":"stream","name":"stderr","text":["2024-04-23 17:36:44,669 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n","2024-04-23 17:36:46,922 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n","2024-04-23 17:36:47,180 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n","2024-04-23 17:36:47,181 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n","2024-04-23 17:36:47,207 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2024-04-23 17:36:47,648 INFO mapred.FileInputFormat: Total input files to process : 1\n","2024-04-23 17:36:47,713 INFO mapreduce.JobSubmitter: number of splits:1\n","2024-04-23 17:36:48,332 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1971985127_0001\n","2024-04-23 17:36:48,332 INFO mapreduce.JobSubmitter: Executing with tokens: []\n","2024-04-23 17:36:48,977 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n","2024-04-23 17:36:48,979 INFO mapreduce.Job: Running job: job_local1971985127_0001\n","2024-04-23 17:36:49,003 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n","2024-04-23 17:36:49,006 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n","2024-04-23 17:36:49,022 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-04-23 17:36:49,026 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-04-23 17:36:49,112 INFO mapred.LocalJobRunner: Waiting for map tasks\n","2024-04-23 17:36:49,121 INFO mapred.LocalJobRunner: Starting task: attempt_local1971985127_0001_m_000000_0\n","2024-04-23 17:36:49,197 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-04-23 17:36:49,200 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-04-23 17:36:49,261 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2024-04-23 17:36:49,289 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n","2024-04-23 17:36:49,326 INFO mapred.MapTask: numReduceTasks: 1\n","2024-04-23 17:36:49,505 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2024-04-23 17:36:49,505 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2024-04-23 17:36:49,505 INFO mapred.MapTask: soft limit at 83886080\n","2024-04-23 17:36:49,505 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2024-04-23 17:36:49,505 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2024-04-23 17:36:49,527 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2024-04-23 17:36:49,540 INFO mapred.LocalJobRunner: \n","2024-04-23 17:36:49,540 INFO mapred.MapTask: Starting flush of map output\n","2024-04-23 17:36:49,540 INFO mapred.MapTask: Spilling map output\n","2024-04-23 17:36:49,540 INFO mapred.MapTask: bufstart = 0; bufend = 22; bufvoid = 104857600\n","2024-04-23 17:36:49,540 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n","2024-04-23 17:36:49,560 INFO mapred.MapTask: Finished spill 0\n","2024-04-23 17:36:49,600 INFO mapred.Task: Task:attempt_local1971985127_0001_m_000000_0 is done. And is in the process of committing\n","2024-04-23 17:36:49,603 INFO mapred.LocalJobRunner: file:/content/hello.txt:0+14\n","2024-04-23 17:36:49,603 INFO mapred.Task: Task 'attempt_local1971985127_0001_m_000000_0' done.\n","2024-04-23 17:36:49,651 INFO mapred.Task: Final Counters for attempt_local1971985127_0001_m_000000_0: Counters: 17\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=141914\n","\t\tFILE: Number of bytes written=855525\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1\n","\t\tMap output records=1\n","\t\tMap output bytes=22\n","\t\tMap output materialized bytes=30\n","\t\tInput split bytes=75\n","\t\tCombine input records=0\n","\t\tSpilled Records=1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=390070272\n","\tFile Input Format Counters \n","\t\tBytes Read=14\n","2024-04-23 17:36:49,652 INFO mapred.LocalJobRunner: Finishing task: attempt_local1971985127_0001_m_000000_0\n","2024-04-23 17:36:49,658 INFO mapred.LocalJobRunner: map task executor complete.\n","2024-04-23 17:36:49,678 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n","2024-04-23 17:36:49,679 INFO mapred.LocalJobRunner: Starting task: attempt_local1971985127_0001_r_000000_0\n","2024-04-23 17:36:49,701 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-04-23 17:36:49,701 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-04-23 17:36:49,702 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2024-04-23 17:36:49,711 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2433f0f6\n","2024-04-23 17:36:49,713 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2024-04-23 17:36:49,773 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n","2024-04-23 17:36:49,794 INFO reduce.EventFetcher: attempt_local1971985127_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n","2024-04-23 17:36:49,873 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1971985127_0001_m_000000_0 decomp: 26 len: 30 to MEMORY\n","2024-04-23 17:36:49,883 INFO reduce.InMemoryMapOutput: Read 26 bytes from map-output for attempt_local1971985127_0001_m_000000_0\n","2024-04-23 17:36:49,895 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 26, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->26\n","2024-04-23 17:36:49,903 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n","2024-04-23 17:36:49,906 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-04-23 17:36:49,906 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n","2024-04-23 17:36:49,918 INFO mapred.Merger: Merging 1 sorted segments\n","2024-04-23 17:36:49,919 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 16 bytes\n","2024-04-23 17:36:49,923 INFO reduce.MergeManagerImpl: Merged 1 segments, 26 bytes to disk to satisfy reduce memory limit\n","2024-04-23 17:36:49,923 INFO reduce.MergeManagerImpl: Merging 1 files, 30 bytes from disk\n","2024-04-23 17:36:49,926 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n","2024-04-23 17:36:49,926 INFO mapred.Merger: Merging 1 sorted segments\n","2024-04-23 17:36:49,928 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 16 bytes\n","2024-04-23 17:36:49,928 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-04-23 17:36:49,941 INFO mapred.Task: Task:attempt_local1971985127_0001_r_000000_0 is done. And is in the process of committing\n","2024-04-23 17:36:49,942 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-04-23 17:36:49,943 INFO mapred.Task: Task attempt_local1971985127_0001_r_000000_0 is allowed to commit now\n","2024-04-23 17:36:49,950 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1971985127_0001_r_000000_0' to file:/content/my_output\n","2024-04-23 17:36:49,953 INFO mapred.LocalJobRunner: reduce > reduce\n","2024-04-23 17:36:49,953 INFO mapred.Task: Task 'attempt_local1971985127_0001_r_000000_0' done.\n","2024-04-23 17:36:49,954 INFO mapred.Task: Final Counters for attempt_local1971985127_0001_r_000000_0: Counters: 24\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=142006\n","\t\tFILE: Number of bytes written=855583\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=1\n","\t\tReduce shuffle bytes=30\n","\t\tReduce input records=1\n","\t\tReduce output records=1\n","\t\tSpilled Records=1\n","\t\tShuffled Maps =1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=1\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=390070272\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Output Format Counters \n","\t\tBytes Written=28\n","2024-04-23 17:36:49,954 INFO mapred.LocalJobRunner: Finishing task: attempt_local1971985127_0001_r_000000_0\n","2024-04-23 17:36:49,954 INFO mapred.LocalJobRunner: reduce task executor complete.\n","2024-04-23 17:36:50,001 INFO mapreduce.Job: Job job_local1971985127_0001 running in uber mode : false\n","2024-04-23 17:36:50,002 INFO mapreduce.Job:  map 100% reduce 100%\n","2024-04-23 17:36:50,004 INFO mapreduce.Job: Job job_local1971985127_0001 completed successfully\n","2024-04-23 17:36:50,065 INFO mapreduce.Job: Counters: 30\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=283920\n","\t\tFILE: Number of bytes written=1711108\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1\n","\t\tMap output records=1\n","\t\tMap output bytes=22\n","\t\tMap output materialized bytes=30\n","\t\tInput split bytes=75\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=1\n","\t\tReduce shuffle bytes=30\n","\t\tReduce input records=1\n","\t\tReduce output records=1\n","\t\tSpilled Records=2\n","\t\tShuffled Maps =1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=1\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=780140544\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Input Format Counters \n","\t\tBytes Read=14\n","\tFile Output Format Counters \n","\t\tBytes Written=28\n","2024-04-23 17:36:50,065 INFO streaming.StreamJob: Output directory: my_output\n"]}]},{"cell_type":"markdown","source":["## Verify the result"],"metadata":{"id":"v7Ks3e96yXuB"}},{"cell_type":"code","source":["%%bash\n","\n","echo \"Check if MapReduce job was successful\"\n","hdfs dfs -test -e my_output/_SUCCESS\n","if [ $? -eq 0 ]; then\n","\techo \"_SUCCESS exists!\"\n","fi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cWAXvG0_yThc","outputId":"f0e91401-16c5-4772-cba6-cc11535720e8","executionInfo":{"status":"ok","timestamp":1713893819227,"user_tz":-420,"elapsed":2276,"user":{"displayName":"","userId":""}}},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Check if MapReduce job was successful\n","_SUCCESS exists!\n"]}]},{"cell_type":"markdown","source":["Show output"],"metadata":{"id":"t40GgJ2Hya9P"}},{"cell_type":"code","source":["!cat my_output/part-00000"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I5APWEgoyaRS","outputId":"6b5a933a-3c55-4d0c-e808-9d05ff9090bf","executionInfo":{"status":"ok","timestamp":1713893821674,"user_tz":-420,"elapsed":366,"user":{"displayName":"","userId":""}}},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["0\tHello, World!\n"]}]},{"cell_type":"markdown","source":["What happened here is that not having defined any mapper or reducer, the \"Identity\" mapper ([IdentityMapper](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/lib/IdentityMapper.html)) and reducer ([IdentityReducer](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/lib/IdentityReducer.html)) were used by default (see [Streaming command options](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Streaming_Command_Options))."],"metadata":{"id":"mzfaMVKqyjpC"}},{"cell_type":"markdown","source":["# Run a map-only MapReduce job\n","\n","Not specifying mapper and reducer in the MapReduce job submission does not mean that MapReduce isn't going to run the mapper and reducer steps, it is simply going to use the Identity mapper and reducer.\n","\n","To run a MapReduce job _without_ reducer one needs to use the generic option\n","\n","    \\-D mapreduce.job.reduces=0\n","\n","(see [specifying map-only jobs](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Specifying_Map-Only_Jobs))."],"metadata":{"id":"lzIuWv7Myndc"}},{"cell_type":"code","source":["%%bash\n","hdfs dfs -rm -r my_output\n","\n","mapred streaming \\\n","    -D mapreduce.job.reduces=0 \\\n","    -input hello.txt \\\n","    -output my_output"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OdwKWyVRye27","outputId":"cc8d1046-3e33-4058-bbbf-6b99b543ae8c","executionInfo":{"status":"ok","timestamp":1713893832967,"user_tz":-420,"elapsed":7500,"user":{"displayName":"","userId":""}}},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Deleted my_output\n"]},{"output_type":"stream","name":"stderr","text":["2024-04-23 17:37:08,797 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n","2024-04-23 17:37:11,005 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n","2024-04-23 17:37:11,182 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n","2024-04-23 17:37:11,182 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n","2024-04-23 17:37:11,205 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2024-04-23 17:37:11,530 INFO mapred.FileInputFormat: Total input files to process : 1\n","2024-04-23 17:37:11,566 INFO mapreduce.JobSubmitter: number of splits:1\n","2024-04-23 17:37:11,909 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local350162101_0001\n","2024-04-23 17:37:11,909 INFO mapreduce.JobSubmitter: Executing with tokens: []\n","2024-04-23 17:37:12,223 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n","2024-04-23 17:37:12,225 INFO mapreduce.Job: Running job: job_local350162101_0001\n","2024-04-23 17:37:12,240 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n","2024-04-23 17:37:12,256 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n","2024-04-23 17:37:12,273 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-04-23 17:37:12,274 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-04-23 17:37:12,389 INFO mapred.LocalJobRunner: Waiting for map tasks\n","2024-04-23 17:37:12,397 INFO mapred.LocalJobRunner: Starting task: attempt_local350162101_0001_m_000000_0\n","2024-04-23 17:37:12,472 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-04-23 17:37:12,482 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-04-23 17:37:12,574 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2024-04-23 17:37:12,587 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n","2024-04-23 17:37:12,609 INFO mapred.MapTask: numReduceTasks: 0\n","2024-04-23 17:37:12,638 INFO mapred.LocalJobRunner: \n","2024-04-23 17:37:12,657 INFO mapred.Task: Task:attempt_local350162101_0001_m_000000_0 is done. And is in the process of committing\n","2024-04-23 17:37:12,665 INFO mapred.LocalJobRunner: \n","2024-04-23 17:37:12,665 INFO mapred.Task: Task attempt_local350162101_0001_m_000000_0 is allowed to commit now\n","2024-04-23 17:37:12,682 INFO output.FileOutputCommitter: Saved output of task 'attempt_local350162101_0001_m_000000_0' to file:/content/my_output\n","2024-04-23 17:37:12,688 INFO mapred.LocalJobRunner: file:/content/hello.txt:0+14\n","2024-04-23 17:37:12,688 INFO mapred.Task: Task 'attempt_local350162101_0001_m_000000_0' done.\n","2024-04-23 17:37:12,705 INFO mapred.Task: Final Counters for attempt_local350162101_0001_m_000000_0: Counters: 15\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=141914\n","\t\tFILE: Number of bytes written=852049\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1\n","\t\tMap output records=1\n","\t\tInput split bytes=75\n","\t\tSpilled Records=0\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=402653184\n","\tFile Input Format Counters \n","\t\tBytes Read=14\n","\tFile Output Format Counters \n","\t\tBytes Written=28\n","2024-04-23 17:37:12,706 INFO mapred.LocalJobRunner: Finishing task: attempt_local350162101_0001_m_000000_0\n","2024-04-23 17:37:12,707 INFO mapred.LocalJobRunner: map task executor complete.\n","2024-04-23 17:37:13,249 INFO mapreduce.Job: Job job_local350162101_0001 running in uber mode : false\n","2024-04-23 17:37:13,252 INFO mapreduce.Job:  map 100% reduce 0%\n","2024-04-23 17:37:13,255 INFO mapreduce.Job: Job job_local350162101_0001 completed successfully\n","2024-04-23 17:37:13,266 INFO mapreduce.Job: Counters: 15\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=141914\n","\t\tFILE: Number of bytes written=852049\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1\n","\t\tMap output records=1\n","\t\tInput split bytes=75\n","\t\tSpilled Records=0\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=402653184\n","\tFile Input Format Counters \n","\t\tBytes Read=14\n","\tFile Output Format Counters \n","\t\tBytes Written=28\n","2024-04-23 17:37:13,266 INFO streaming.StreamJob: Output directory: my_output\n"]}]},{"cell_type":"markdown","source":["## Verify the result"],"metadata":{"id":"QZIE9yXOyyHJ"}},{"cell_type":"code","source":["!hdfs dfs -test -e my_output/_SUCCESS && cat my_output/part-00000"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Dt3tUI0yu5e","outputId":"87a7b179-90f5-478f-83e0-af6d6d887f82","executionInfo":{"status":"ok","timestamp":1713893840198,"user_tz":-420,"elapsed":2738,"user":{"displayName":"","userId":""}}},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["0\tHello, World!\n"]}]},{"cell_type":"markdown","source":["## Why a map-only application?\n","\n","The advantage of a map-only job is that the sorting and shuffling phases are skipped, so if you do not need that remember to specify `-D mapreduce.job.reduces=0 `.\n","\n","On the other hand, a MapReduce job even with the default `IdentityReducer` will deliver sorted results because the data passed from the mapper to the reducer always gets sorted.\n"],"metadata":{"id":"hUGEUv99y3cM"}},{"cell_type":"markdown","source":["# Improved version of the MapReduce \"Hello, World!\" application\n","\n","Taking into account the previous considerations, here's a more efficient version of the 'Hello, World!' application that bypasses the shuffling and sorting step."],"metadata":{"id":"FhVVFEdKzGcI"}},{"cell_type":"code","source":["%%bash\n","hdfs dfs -rm -r my_output\n","\n","mapred streaming \\\n","    -D mapreduce.job.reduces=0 \\\n","    -input hello.txt \\\n","    -output my_output \\\n","    -mapper '/bin/cat'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jLgMXX2jy0vC","outputId":"3c04ace9-7930-4308-c7a9-4ec5f1c9034c","executionInfo":{"status":"ok","timestamp":1713893851201,"user_tz":-420,"elapsed":6663,"user":{"displayName":"","userId":""}}},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Deleted my_output\n"]},{"output_type":"stream","name":"stderr","text":["2024-04-23 17:37:27,880 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n","2024-04-23 17:37:30,015 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n","2024-04-23 17:37:30,200 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n","2024-04-23 17:37:30,200 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n","2024-04-23 17:37:30,224 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2024-04-23 17:37:30,473 INFO mapred.FileInputFormat: Total input files to process : 1\n","2024-04-23 17:37:30,499 INFO mapreduce.JobSubmitter: number of splits:1\n","2024-04-23 17:37:30,825 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local566471422_0001\n","2024-04-23 17:37:30,825 INFO mapreduce.JobSubmitter: Executing with tokens: []\n","2024-04-23 17:37:31,115 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n","2024-04-23 17:37:31,117 INFO mapreduce.Job: Running job: job_local566471422_0001\n","2024-04-23 17:37:31,130 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n","2024-04-23 17:37:31,134 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n","2024-04-23 17:37:31,142 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-04-23 17:37:31,143 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-04-23 17:37:31,221 INFO mapred.LocalJobRunner: Waiting for map tasks\n","2024-04-23 17:37:31,227 INFO mapred.LocalJobRunner: Starting task: attempt_local566471422_0001_m_000000_0\n","2024-04-23 17:37:31,270 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-04-23 17:37:31,273 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-04-23 17:37:31,308 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2024-04-23 17:37:31,322 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n","2024-04-23 17:37:31,340 INFO mapred.MapTask: numReduceTasks: 0\n","2024-04-23 17:37:31,355 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n","2024-04-23 17:37:31,361 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n","2024-04-23 17:37:31,363 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n","2024-04-23 17:37:31,364 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n","2024-04-23 17:37:31,364 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n","2024-04-23 17:37:31,364 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n","2024-04-23 17:37:31,365 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n","2024-04-23 17:37:31,366 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n","2024-04-23 17:37:31,366 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n","2024-04-23 17:37:31,366 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n","2024-04-23 17:37:31,367 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n","2024-04-23 17:37:31,368 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n","2024-04-23 17:37:31,368 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n","2024-04-23 17:37:31,411 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2024-04-23 17:37:31,413 INFO streaming.PipeMapRed: Records R/W=1/1\n","2024-04-23 17:37:31,417 INFO streaming.PipeMapRed: MRErrorThread done\n","2024-04-23 17:37:31,418 INFO streaming.PipeMapRed: mapRedFinished\n","2024-04-23 17:37:31,424 INFO mapred.LocalJobRunner: \n","2024-04-23 17:37:31,437 INFO mapred.Task: Task:attempt_local566471422_0001_m_000000_0 is done. And is in the process of committing\n","2024-04-23 17:37:31,440 INFO mapred.LocalJobRunner: \n","2024-04-23 17:37:31,440 INFO mapred.Task: Task attempt_local566471422_0001_m_000000_0 is allowed to commit now\n","2024-04-23 17:37:31,442 INFO output.FileOutputCommitter: Saved output of task 'attempt_local566471422_0001_m_000000_0' to file:/content/my_output\n","2024-04-23 17:37:31,443 INFO mapred.LocalJobRunner: Records R/W=1/1\n","2024-04-23 17:37:31,444 INFO mapred.Task: Task 'attempt_local566471422_0001_m_000000_0' done.\n","2024-04-23 17:37:31,454 INFO mapred.Task: Final Counters for attempt_local566471422_0001_m_000000_0: Counters: 15\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=141914\n","\t\tFILE: Number of bytes written=855001\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1\n","\t\tMap output records=1\n","\t\tInput split bytes=75\n","\t\tSpilled Records=0\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=20\n","\t\tTotal committed heap usage (bytes)=420478976\n","\tFile Input Format Counters \n","\t\tBytes Read=14\n","\tFile Output Format Counters \n","\t\tBytes Written=27\n","2024-04-23 17:37:31,454 INFO mapred.LocalJobRunner: Finishing task: attempt_local566471422_0001_m_000000_0\n","2024-04-23 17:37:31,455 INFO mapred.LocalJobRunner: map task executor complete.\n","2024-04-23 17:37:32,128 INFO mapreduce.Job: Job job_local566471422_0001 running in uber mode : false\n","2024-04-23 17:37:32,131 INFO mapreduce.Job:  map 100% reduce 0%\n","2024-04-23 17:37:32,135 INFO mapreduce.Job: Job job_local566471422_0001 completed successfully\n","2024-04-23 17:37:32,148 INFO mapreduce.Job: Counters: 15\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=141914\n","\t\tFILE: Number of bytes written=855001\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1\n","\t\tMap output records=1\n","\t\tInput split bytes=75\n","\t\tSpilled Records=0\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=20\n","\t\tTotal committed heap usage (bytes)=420478976\n","\tFile Input Format Counters \n","\t\tBytes Read=14\n","\tFile Output Format Counters \n","\t\tBytes Written=27\n","2024-04-23 17:37:32,148 INFO streaming.StreamJob: Output directory: my_output\n"]}]},{"cell_type":"code","source":["!hdfs dfs -test -e my_output/_SUCCESS && cat my_output/part-00000"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sa1UDPr6zKKw","outputId":"a804898c-958e-4c56-b633-25bf0f0ce3ca","executionInfo":{"status":"ok","timestamp":1713893860625,"user_tz":-420,"elapsed":2769,"user":{"displayName":"","userId":""}}},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Hello, World!\t\n"]}]}]}