{"cells":[{"cell_type":"markdown","metadata":{"id":"zp5DduDsD3FB"},"source":["# Mapreduce with bash\n","\n","In this notebook we're going to use `bash` to write a mapper and a reducer to count words in a file. This example will serve to illustrate the main features of Hadoop's MapReduce framework."]},{"cell_type":"markdown","metadata":{"id":"HsUM_0rlD3FF"},"source":["# Table of contents\n","- [What is MapReduce?](#mapreduce)\n","- [The mapper](#mapper)\n","    - [Test the mapper](#testmapper)\n","- [Hadoop it up](#hadoop)\n","    - [What is Hadoop Streaming?](#hadoopstreaming)\n","    - [List your Hadoop directory](#hdfs_ls)\n","    - [Test MapReduce with a dummy reducer](#dummyreducer)\n","    - [Shuffling and sorting](#shuffling&sorting)\n","- [The reducer](#reducer)\n","    - [Test and run](#run)\n","- [Run a mapreduce job with more data](#moredata)\n","    - [Sort the output with `sort`](#sortoutput)\n","    - [Sort the output with another MapReduce job](#sortoutputMR)\n","    - [Configure sort with `KeyFieldBasedComparator`](#KeyFieldBasedComparator)\n","    - [Specifying Configuration Variables with the -D Option](#configuration_variables)\n","    - [What is word count useful for?](#wordcount)\n"]},{"cell_type":"markdown","metadata":{"id":"7H7GhxImD3FH"},"source":["## What is MapReduce? <a name=\"mapreduce\"></a>\n","\n","MapReduce is a computing paradigm designed to allow parallel distributed processing of massive amounts of data.\n","\n","Data is split across several computer nodes, there it is processed by one or more mappers. The results emitted by the mappers are first sorted and then passed to one or more reducers that process and combine the data to return the final result.\n","\n","![Map & Reduce](mapreduce.png)\n","With [Hadoop Streaming](https://hadoop.apache.org/docs/current/hadoop-streaming/HadoopStreaming.html) it is possible to use any programming language to define a mapper and/or a reducer. Here we're going to use the Unix `bash` scripting language ([here](https://www.gnu.org/software/bash/manual/html_node/index.html) is the official documentation for the language)."]},{"cell_type":"markdown","metadata":{"id":"aBN5dpwfD3FI"},"source":["## The mapper <a name=\"mapper\"></a>\n","Let's write a mapper script called `map.sh`. The mapper splits each input line into words and for each word it outputs a line containing the word and `1` separated by a tab.\n","\n","Example: for the input\n","<html>\n","<pre>\n","apple orange\n","banana apple peach\n","</pre>\n","</html>\n","\n","`map.sh` outputs:\n","<html>\n","<pre>\n","apple   1\n","orange  1\n","banana  1\n","apple  1\n","peach  1\n","</pre>\n","</html>\n","\n","\n","The <a href=\"https://ipython.readthedocs.io/en/stable/interactive/magics.html\">_cell magic_</a> [`%%writefile`](https://ipython.readthedocs.io/en/stable/interactive/magics.html#cellmagic-writefile) allows us to write the contents of the cell to a file."]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p9YlfOSBD3FJ","executionInfo":{"status":"ok","timestamp":1713895167750,"user_tz":-420,"elapsed":377,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}},"outputId":"727f2dbb-ea0e-4fcb-df9b-395245bee545"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting map.sh\n"]}],"source":["%%writefile map.sh\n","#!/bin/bash\n","\n","while read line\n","do\n"," for word in $line\n"," do\n","  if [ -n \"$word\" ]\n","  then\n","     echo -e ${word}\"\\t1\"\n","  fi\n"," done\n","done"]},{"cell_type":"markdown","metadata":{"id":"n1jR1XrtD3FK"},"source":["After running the cell above, you should have a new file `map.sh` in your current directory.\n","The file can be seen in the left panel of JupyterLab or by using a list command on the bash command-line.\n","\n","**Note:** you can execute a single bash command in a Jupyter notebook cell by prepending an exclamation point to the command."]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DbPU5lxuD3FK","executionInfo":{"status":"ok","timestamp":1713895168861,"user_tz":-420,"elapsed":728,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}},"outputId":"f8ac8967-87ac-4fb2-ee7e-b0c5c3d2772f"},"outputs":[{"output_type":"stream","name":"stdout","text":["-rwx------ 1 root root 126 Apr 23 17:59 map.sh\n"]}],"source":["!ls -hl map.sh"]},{"cell_type":"markdown","metadata":{"id":"Ix9IGABpD3FL"},"source":["### Test the mapper <a name=\"testmapper\"></a>\n","We're going to test the mapper on on the command line with a small text file `fruits.txt` by first creating the text file.\n","In this file `apple` for instance appears two times, that's what we want our mapreduce job to compute."]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NcgqEXw5D3FL","executionInfo":{"status":"ok","timestamp":1713895168861,"user_tz":-420,"elapsed":9,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}},"outputId":"ca2f8013-51dd-4770-a9dc-21917d69b3e3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting fruits.txt\n"]}],"source":["%%writefile fruits.txt\n","apple banana\n","peach orange peach peach\n","pineapple peach apple"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UgOUZXxvD3FL","executionInfo":{"status":"ok","timestamp":1713895168861,"user_tz":-420,"elapsed":7,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}},"outputId":"6c700695-a02b-4ebc-89f8-a4e7921cb5dd"},"outputs":[{"output_type":"stream","name":"stdout","text":["apple banana\n","peach orange peach peach\n","pineapple peach apple\n"]}],"source":["!cat fruits.txt"]},{"cell_type":"markdown","metadata":{"id":"hQFCbVD7D3FM"},"source":["Test the mapper"]},{"cell_type":"code","source":["!chmod +x map.sh"],"metadata":{"id":"nBcalfBCELf7","executionInfo":{"status":"ok","timestamp":1713895168861,"user_tz":-420,"elapsed":6,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8vqlbtWND3FM","executionInfo":{"status":"ok","timestamp":1713895168861,"user_tz":-420,"elapsed":5,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}},"outputId":"746b0044-8358-4348-ae8f-9538d750be35"},"outputs":[{"output_type":"stream","name":"stdout","text":["apple\t1\n","banana\t1\n","peach\t1\n","orange\t1\n","peach\t1\n","peach\t1\n","pineapple\t1\n","peach\t1\n","apple\t1\n"]}],"source":["!cat fruits.txt|./map.sh"]},{"cell_type":"markdown","metadata":{"id":"3cGxPiebD3FN"},"source":["If the script `map.sh` does not have the executable bit set, you need to set the correct permissions."]},{"cell_type":"code","execution_count":28,"metadata":{"id":"f-LJFX_8D3FN","executionInfo":{"status":"ok","timestamp":1713895168861,"user_tz":-420,"elapsed":3,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}}},"outputs":[],"source":["!chmod 700 map.sh"]},{"cell_type":"markdown","metadata":{"id":"OD_oCJq1D3FO"},"source":["## Hadoop it up <a name=\"hadoop\"></a>\n","Let us now run a MapReduce job with Hadoop Streaming."]},{"cell_type":"code","source":["HADOOP_URL = \"https://dlcdn.apache.org/hadoop/common/stable/hadoop-3.4.0.tar.gz\"\n","\n","import requests\n","import os\n","import tarfile\n","\n","def download_and_extract_targz(url):\n","    response = requests.get(url)\n","    filename = url.rsplit('/', 1)[-1]\n","    HADOOP_HOME = filename[:-7]\n","    # set HADOOP_HOME environment variable\n","    os.environ['HADOOP_HOME'] = HADOOP_HOME\n","    if os.path.isdir(HADOOP_HOME):\n","      print(\"Not downloading, Hadoop folder {} already exists\".format(HADOOP_HOME))\n","      return\n","    if response.status_code == 200:\n","        with open(filename, 'wb') as file:\n","            file.write(response.content)\n","        with tarfile.open(filename, 'r:gz') as tar_ref:\n","            extract_path = tar_ref.extractall(path='.')\n","            # Get the names of all members (files and directories) in the archive\n","            all_members = tar_ref.getnames()\n","            # If there is a top-level directory, get its name\n","            if all_members:\n","              top_level_directory = all_members[0]\n","              print(f\"ZIP file downloaded and extracted successfully. Contents saved at: {top_level_directory}\")\n","    else:\n","        print(f\"Failed to download ZIP file. Status code: {response.status_code}\")\n","\n","\n","download_and_extract_targz(HADOOP_URL)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VWr2LbuuGk8a","executionInfo":{"status":"ok","timestamp":1713895235524,"user_tz":-420,"elapsed":66666,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}},"outputId":"10e92440-1baa-4949-cd82-5a4e0a94d5d3"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["ZIP file downloaded and extracted successfully. Contents saved at: hadoop-3.4.0\n"]}]},{"cell_type":"code","source":["\n","# HADOOP_HOME was set earlier when downloading Hadoop distribution\n","print(\"HADOOP_HOME is {}\".format(os.environ['HADOOP_HOME']))\n","\n","os.environ['PATH'] = ':'.join([os.path.join(os.environ['HADOOP_HOME'], 'bin'), os.environ['PATH']])\n","print(\"PATH is {}\".format(os.environ['PATH']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j4I0stzyHFvZ","executionInfo":{"status":"ok","timestamp":1713895235524,"user_tz":-420,"elapsed":44,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}},"outputId":"d4c8161e-809a-42dc-b22c-bba1aca95339"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["HADOOP_HOME is hadoop-3.4.0\n","PATH is hadoop-3.4.0/bin:/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"]}]},{"cell_type":"code","source":["import shutil\n","\n","# set variable JAVA_HOME (install Java if necessary)\n","def is_java_installed():\n","    os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n","    return os.environ['JAVA_HOME']\n","\n","def install_java():\n","    # Uncomment and modify the desired version\n","    # java_version= 'openjdk-11-jre-headless'\n","    # java_version= 'default-jre'\n","    # java_version= 'openjdk-17-jre-headless'\n","    # java_version= 'openjdk-18-jre-headless'\n","    java_version= 'openjdk-19-jre-headless'\n","\n","    print(f\"Java not found. Installing {java_version} ... (this might take a while)\")\n","    try:\n","        cmd = f\"apt install -y {java_version}\"\n","        subprocess_output = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n","        stdout_result = subprocess_output.stdout\n","        # Process the results as needed\n","        print(\"Done installing Java {}\".format(java_version))\n","        os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n","        print(\"JAVA_HOME is {}\".format(os.environ['JAVA_HOME']))\n","    except subprocess.CalledProcessError as e:\n","        # Handle the error if the command returns a non-zero exit code\n","        print(\"Command failed with return code {}\".format(e.returncode))\n","        print(\"stdout: {}\".format(e.stdout))\n","\n","# Install Java if not available\n","if is_java_installed():\n","    print(\"Java is already installed: {}\".format(os.environ['JAVA_HOME']))\n","else:\n","    print(\"Installing Java\")\n","    install_java()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E3_3ywvTHIlo","executionInfo":{"status":"ok","timestamp":1713895235525,"user_tz":-420,"elapsed":39,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}},"outputId":"2329f988-b27b-46b5-8069-3e146e6fcad5"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Java is already installed: /usr/lib/jvm/java-11-openjdk-amd64\n"]}]},{"cell_type":"markdown","metadata":{"id":"RXsP40sHD3FO"},"source":["### What is Hadoop Streaming <a name=\"hadoopstreaming\"></a>\n","\n","Hadoop Streaming is a library included in the Hadoop distribution that enables you to develop MapReduce executables in languages other than Java.\n","\n","Mapper and/or reducer can be any sort of executables that read the input from stdin and emit the output to stdout. By default, input is read line by line and the prefix of a line up to the first tab character is the key; the rest of the line (excluding the tab character) will be the value.\n","\n","If there is no tab character in the line, then the entire line is considered as key and the value is null. The default input format is specified in the class `TextInputFormat` (see the [API documentation](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/TextInputFormat.html)) but this can can be customized for instance by defining another field separator (see the [Hadoop Streaming documentation](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Customizing_How_Lines_are_Split_into_KeyValue_Pairs).\n","\n","This is an example of MapReduce streaming invocation syntax:\n","<html>\n","<pre>\n","    mapred streaming \\\n","  -input myInputDirs \\\n","  -output myOutputDir \\\n","  -mapper /bin/cat \\\n","  -reducer /usr/bin/wc\n","\n","</pre>\n","</html>\n","\n","You can find the full official documentation for Hadoop Streaming from Apache Hadoop here: [https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html).\n","\n","All options for the Hadoop Streaming command are described here: [Streaming Command Options](https://hadoop.apache.org/docs/current/hadoop-streaming/HadoopStreaming.html#Streaming_Command_Options) and can be listed with the command"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZB78UK9oD3FO","executionInfo":{"status":"ok","timestamp":1713895236626,"user_tz":-420,"elapsed":1137,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}},"outputId":"58f233c6-893f-42e7-83f5-dcb3eb66b69f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Usage: $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar [options]\n","Options:\n","  -input          <path> DFS input file(s) for the Map step.\n","  -output         <path> DFS output directory for the Reduce step.\n","  -mapper         <cmd|JavaClassName> Optional. Command to be run as mapper.\n","  -combiner       <cmd|JavaClassName> Optional. Command to be run as combiner.\n","  -reducer        <cmd|JavaClassName> Optional. Command to be run as reducer.\n","  -file           <file> Optional. File/dir to be shipped in the Job jar file.\n","                  Deprecated. Use generic option \"-files\" instead.\n","  -inputformat    <TextInputFormat(default)|SequenceFileAsTextInputFormat|JavaClassName>\n","                  Optional. The input format class.\n","  -outputformat   <TextOutputFormat(default)|JavaClassName>\n","                  Optional. The output format class.\n","  -partitioner    <JavaClassName>  Optional. The partitioner class.\n","  -numReduceTasks <num> Optional. Number of reduce tasks.\n","  -inputreader    <spec> Optional. Input recordreader spec.\n","  -cmdenv         <n>=<v> Optional. Pass env.var to streaming commands.\n","  -mapdebug       <cmd> Optional. To run this script when a map task fails.\n","  -reducedebug    <cmd> Optional. To run this script when a reduce task fails.\n","  -io             <identifier> Optional. Format to use for input to and output\n","                  from mapper/reducer commands\n","  -lazyOutput     Optional. Lazily create Output.\n","  -background     Optional. Submit the job and don't wait till it completes.\n","  -verbose        Optional. Print verbose output.\n","  -info           Optional. Print detailed usage.\n","  -help           Optional. Print help message.\n","\n","Generic options supported are:\n","-conf <configuration file>        specify an application configuration file\n","-D <property=value>               define a value for a given property\n","-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\n","-jt <local|resourcemanager:port>  specify a ResourceManager\n","-files <file1,...>                specify a comma-separated list of files to be copied to the map reduce cluster\n","-libjars <jar1,...>               specify a comma-separated list of jar files to be included in the classpath\n","-archives <archive1,...>          specify a comma-separated list of archives to be unarchived on the compute machines\n","\n","The general command line syntax is:\n","command [genericOptions] [commandOptions]\n","\n","\n","For more details about these options:\n","Use $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar -info\n"]}],"source":["!mapred streaming --help"]},{"cell_type":"markdown","metadata":{"id":"k9-801XPD3FO"},"source":["Now in order to run a mapreduce job that we need to \"upload\" the input file to the Hadoop file system."]},{"cell_type":"markdown","metadata":{"id":"T_3qJpg_D3FO"},"source":["### List your Hadoop directory <a name=\"hdfs_ls\"></a>\n","\n","With the command `hdfs dfs -l` you can view the content of your HDFS home directory.\n","\n","`hdfs dfs` you can run a filesystem command on the Hadoop fileystem. The complete list of commands can be found in the [System Shell Guide](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html#dfs)."]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hdGlKZ_ND3FO","executionInfo":{"status":"ok","timestamp":1713895239841,"user_tz":-420,"elapsed":3221,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}},"outputId":"2dd62a1d-f0e9-438d-ca0e-d7991e7d5e39"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 8 items\n","drwxr-xr-x   - root root       4096 2024-04-22 13:25 .config\n","-rw-r--r--   1 root root         60 2024-04-23 17:59 fruits.txt\n","drwxr-xr-x   - 1000 1000       4096 2023-06-18 09:08 hadoop-3.3.6\n","-rw-r--r--   1 root root  730107476 2023-06-25 23:35 hadoop-3.3.6.tar.gz\n","drwxr-xr-x   - root root       4096 2024-03-04 08:05 hadoop-3.4.0\n","-rw-r--r--   1 root root  965537117 2024-04-23 18:00 hadoop-3.4.0.tar.gz\n","-rwx------   1 root root        126 2024-04-23 17:59 map.sh\n","drwxr-xr-x   - root root       4096 2024-04-22 13:25 sample_data\n"]}],"source":["!hdfs dfs -ls"]},{"cell_type":"markdown","metadata":{"id":"rHanZjE3D3FP"},"source":["Now create a directory `wordcount` with a subdirectory `input` on the Hadoop filesystem."]},{"cell_type":"code","execution_count":34,"metadata":{"id":"dmt_6ylDD3FP","executionInfo":{"status":"ok","timestamp":1713895242820,"user_tz":-420,"elapsed":3008,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}}},"outputs":[],"source":["%%bash\n","hdfs dfs -mkdir -p wordcount"]},{"cell_type":"markdown","metadata":{"id":"eKXvmij-D3FP"},"source":["Copy the file fruits.txt to Hadoop in the folder `wordcount/input`.\n","\n","Why do we need this step? Because the file `fruits.txt` needs to reside on the Hadoop filesystem in order to enjoy of all of the features of Hadoop (data partitioning, distributed processing, fault tolerance)."]},{"cell_type":"code","execution_count":35,"metadata":{"id":"GVqFURuyD3FP","executionInfo":{"status":"ok","timestamp":1713895248836,"user_tz":-420,"elapsed":6021,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}}},"outputs":[],"source":["%%bash\n","hdfs dfs -rm -r wordcount/input 2>/dev/null\n","hdfs dfs -mkdir wordcount/input\n","hdfs dfs -put fruits.txt wordcount/input"]},{"cell_type":"markdown","metadata":{"id":"Qucg8xRRD3FP"},"source":["Let's check if the file is there now.\n","\n","**Note:** it is convenient use the option `-h` for `ls` to show file sizes in human-readable form (showing sizes in Kilobytes, Megabytes, Gigabytes, etc.)"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XOtBLiJCD3FP","executionInfo":{"status":"ok","timestamp":1713895253942,"user_tz":-420,"elapsed":5132,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}},"outputId":"f55c758b-d51f-49ae-801c-2aacb1675f55"},"outputs":[{"output_type":"stream","name":"stdout","text":["-rw-r--r--   1 root root         60 2024-04-23 18:00 wordcount/input/fruits.txt\n"]}],"source":["!hdfs dfs -ls -h -R wordcount/input"]},{"cell_type":"markdown","metadata":{"id":"mUrZeJN5D3FP"},"source":["### Test MapReduce with a dummy reducer <a name=\"dummyreducer\"></a>\n","\n","Let's try to run the mapper using a dummy reducer (`/bin/cat` does nothing else than echoing the data it receives).\n","\n","**Warning:** mapreduce tends to produce a verbose output, so be ready to see a long output. What you should look for is a message of the kind <html><pre>\"INFO mapreduce.Job: Job ... completed successfully\"</pre></html>\n","\n","**Note:** at the beginning of next cell you'll see a command `hadoop fs -rmr wordcount/output 2>/dev/null`. This is needed because when you run a job several times mapreduce will give an error if you try to overwrite the same output directory."]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ut5tsrcED3FQ","executionInfo":{"status":"ok","timestamp":1713895261593,"user_tz":-420,"elapsed":7664,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}},"outputId":"0a7c8cea-831e-4fd1-f0a8-51e9d2ee821e"},"outputs":[{"output_type":"stream","name":"stderr","text":["2024-04-23 18:00:59,235 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n","2024-04-23 18:00:59,534 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n","2024-04-23 18:00:59,534 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n","2024-04-23 18:00:59,562 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2024-04-23 18:00:59,967 INFO mapred.FileInputFormat: Total input files to process : 1\n","2024-04-23 18:01:00,005 INFO mapreduce.JobSubmitter: number of splits:1\n","2024-04-23 18:01:00,607 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local847037377_0001\n","2024-04-23 18:01:00,607 INFO mapreduce.JobSubmitter: Executing with tokens: []\n","2024-04-23 18:01:01,171 INFO mapred.LocalDistributedCacheManager: Localized file:/content/map.sh as file:/tmp/hadoop-root/mapred/local/job_local847037377_0001_24d518a3-270e-46f4-af0b-4c707baa2d4e/map.sh\n","2024-04-23 18:01:01,327 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n","2024-04-23 18:01:01,329 INFO mapreduce.Job: Running job: job_local847037377_0001\n","2024-04-23 18:01:01,338 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n","2024-04-23 18:01:01,341 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n","2024-04-23 18:01:01,363 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-04-23 18:01:01,363 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-04-23 18:01:01,489 INFO mapred.LocalJobRunner: Waiting for map tasks\n","2024-04-23 18:01:01,497 INFO mapred.LocalJobRunner: Starting task: attempt_local847037377_0001_m_000000_0\n","2024-04-23 18:01:01,571 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-04-23 18:01:01,572 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-04-23 18:01:01,600 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2024-04-23 18:01:01,616 INFO mapred.MapTask: Processing split: file:/content/wordcount/input/fruits.txt:0+60\n","2024-04-23 18:01:01,640 INFO mapred.MapTask: numReduceTasks: 1\n","2024-04-23 18:01:01,754 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2024-04-23 18:01:01,754 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2024-04-23 18:01:01,754 INFO mapred.MapTask: soft limit at 83886080\n","2024-04-23 18:01:01,754 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2024-04-23 18:01:01,754 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2024-04-23 18:01:01,760 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2024-04-23 18:01:01,777 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./map.sh]\n","2024-04-23 18:01:01,786 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n","2024-04-23 18:01:01,789 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n","2024-04-23 18:01:01,790 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n","2024-04-23 18:01:01,791 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n","2024-04-23 18:01:01,797 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n","2024-04-23 18:01:01,798 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n","2024-04-23 18:01:01,800 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n","2024-04-23 18:01:01,801 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n","2024-04-23 18:01:01,802 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n","2024-04-23 18:01:01,803 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n","2024-04-23 18:01:01,804 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n","2024-04-23 18:01:01,805 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n","2024-04-23 18:01:01,836 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2024-04-23 18:01:01,842 INFO streaming.PipeMapRed: Records R/W=3/1\n","2024-04-23 18:01:01,849 INFO streaming.PipeMapRed: MRErrorThread done\n","2024-04-23 18:01:01,849 INFO streaming.PipeMapRed: mapRedFinished\n","2024-04-23 18:01:01,856 INFO mapred.LocalJobRunner: \n","2024-04-23 18:01:01,856 INFO mapred.MapTask: Starting flush of map output\n","2024-04-23 18:01:01,856 INFO mapred.MapTask: Spilling map output\n","2024-04-23 18:01:01,856 INFO mapred.MapTask: bufstart = 0; bufend = 78; bufvoid = 104857600\n","2024-04-23 18:01:01,856 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214364(104857456); length = 33/6553600\n","2024-04-23 18:01:01,873 INFO mapred.MapTask: Finished spill 0\n","2024-04-23 18:01:01,890 INFO mapred.Task: Task:attempt_local847037377_0001_m_000000_0 is done. And is in the process of committing\n","2024-04-23 18:01:01,895 INFO mapred.LocalJobRunner: Records R/W=3/1\n","2024-04-23 18:01:01,895 INFO mapred.Task: Task 'attempt_local847037377_0001_m_000000_0' done.\n","2024-04-23 18:01:01,904 INFO mapred.Task: Final Counters for attempt_local847037377_0001_m_000000_0: Counters: 17\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=142119\n","\t\tFILE: Number of bytes written=858340\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=3\n","\t\tMap output records=9\n","\t\tMap output bytes=78\n","\t\tMap output materialized bytes=102\n","\t\tInput split bytes=92\n","\t\tCombine input records=0\n","\t\tSpilled Records=9\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=381681664\n","\tFile Input Format Counters \n","\t\tBytes Read=76\n","2024-04-23 18:01:01,904 INFO mapred.LocalJobRunner: Finishing task: attempt_local847037377_0001_m_000000_0\n","2024-04-23 18:01:01,904 INFO mapred.LocalJobRunner: map task executor complete.\n","2024-04-23 18:01:01,911 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n","2024-04-23 18:01:01,911 INFO mapred.LocalJobRunner: Starting task: attempt_local847037377_0001_r_000000_0\n","2024-04-23 18:01:01,925 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-04-23 18:01:01,925 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-04-23 18:01:01,925 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2024-04-23 18:01:01,931 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@33a331a2\n","2024-04-23 18:01:01,937 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2024-04-23 18:01:01,999 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n","2024-04-23 18:01:02,018 INFO reduce.EventFetcher: attempt_local847037377_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n","2024-04-23 18:01:02,083 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local847037377_0001_m_000000_0 decomp: 98 len: 102 to MEMORY\n","2024-04-23 18:01:02,089 INFO reduce.InMemoryMapOutput: Read 98 bytes from map-output for attempt_local847037377_0001_m_000000_0\n","2024-04-23 18:01:02,091 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 98, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->98\n","2024-04-23 18:01:02,095 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n","2024-04-23 18:01:02,097 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-04-23 18:01:02,097 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n","2024-04-23 18:01:02,105 INFO mapred.Merger: Merging 1 sorted segments\n","2024-04-23 18:01:02,106 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 90 bytes\n","2024-04-23 18:01:02,108 INFO reduce.MergeManagerImpl: Merged 1 segments, 98 bytes to disk to satisfy reduce memory limit\n","2024-04-23 18:01:02,110 INFO reduce.MergeManagerImpl: Merging 1 files, 102 bytes from disk\n","2024-04-23 18:01:02,111 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n","2024-04-23 18:01:02,111 INFO mapred.Merger: Merging 1 sorted segments\n","2024-04-23 18:01:02,113 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 90 bytes\n","2024-04-23 18:01:02,113 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-04-23 18:01:02,115 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n","2024-04-23 18:01:02,121 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n","2024-04-23 18:01:02,125 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n","2024-04-23 18:01:02,142 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2024-04-23 18:01:02,146 INFO streaming.PipeMapRed: MRErrorThread done\n","2024-04-23 18:01:02,154 INFO streaming.PipeMapRed: Records R/W=9/1\n","2024-04-23 18:01:02,154 INFO streaming.PipeMapRed: mapRedFinished\n","2024-04-23 18:01:02,156 INFO mapred.Task: Task:attempt_local847037377_0001_r_000000_0 is done. And is in the process of committing\n","2024-04-23 18:01:02,157 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-04-23 18:01:02,157 INFO mapred.Task: Task attempt_local847037377_0001_r_000000_0 is allowed to commit now\n","2024-04-23 18:01:02,162 INFO output.FileOutputCommitter: Saved output of task 'attempt_local847037377_0001_r_000000_0' to file:/content/wordcount/output\n","2024-04-23 18:01:02,165 INFO mapred.LocalJobRunner: Records R/W=9/1 > reduce\n","2024-04-23 18:01:02,165 INFO mapred.Task: Task 'attempt_local847037377_0001_r_000000_0' done.\n","2024-04-23 18:01:02,166 INFO mapred.Task: Final Counters for attempt_local847037377_0001_r_000000_0: Counters: 24\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=142355\n","\t\tFILE: Number of bytes written=858532\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=5\n","\t\tReduce shuffle bytes=102\n","\t\tReduce input records=9\n","\t\tReduce output records=9\n","\t\tSpilled Records=9\n","\t\tShuffled Maps =1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=1\n","\t\tGC time elapsed (ms)=20\n","\t\tTotal committed heap usage (bytes)=381681664\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Output Format Counters \n","\t\tBytes Written=90\n","2024-04-23 18:01:02,167 INFO mapred.LocalJobRunner: Finishing task: attempt_local847037377_0001_r_000000_0\n","2024-04-23 18:01:02,167 INFO mapred.LocalJobRunner: reduce task executor complete.\n","2024-04-23 18:01:02,336 INFO mapreduce.Job: Job job_local847037377_0001 running in uber mode : false\n","2024-04-23 18:01:02,337 INFO mapreduce.Job:  map 100% reduce 100%\n","2024-04-23 18:01:02,339 INFO mapreduce.Job: Job job_local847037377_0001 completed successfully\n","2024-04-23 18:01:02,349 INFO mapreduce.Job: Counters: 30\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=284474\n","\t\tFILE: Number of bytes written=1716872\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=3\n","\t\tMap output records=9\n","\t\tMap output bytes=78\n","\t\tMap output materialized bytes=102\n","\t\tInput split bytes=92\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=5\n","\t\tReduce shuffle bytes=102\n","\t\tReduce input records=9\n","\t\tReduce output records=9\n","\t\tSpilled Records=18\n","\t\tShuffled Maps =1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=1\n","\t\tGC time elapsed (ms)=20\n","\t\tTotal committed heap usage (bytes)=763363328\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Input Format Counters \n","\t\tBytes Read=76\n","\tFile Output Format Counters \n","\t\tBytes Written=90\n","2024-04-23 18:01:02,350 INFO streaming.StreamJob: Output directory: wordcount/output\n"]}],"source":["%%bash\n","hdfs dfs -rm -r wordcount/output 2>/dev/null\n","mapred streaming \\\n","  -files map.sh \\\n","  -input wordcount/input \\\n","  -output wordcount/output \\\n","  -mapper map.sh \\\n","  -reducer /bin/cat"]},{"cell_type":"markdown","metadata":{"id":"aiflR496D3FR"},"source":["The output of the mapreduce job is in the `output` subfolder of the input directory. Let's check what's inside it."]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HXE5elH9D3FR","executionInfo":{"status":"ok","timestamp":1713895263525,"user_tz":-420,"elapsed":1935,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}},"outputId":"8246fea5-17d8-4e5f-ba96-3bc7a4a872c3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 2 items\n","-rw-r--r--   1 root root          0 2024-04-23 18:01 wordcount/output/_SUCCESS\n","-rw-r--r--   1 root root         78 2024-04-23 18:01 wordcount/output/part-00000\n"]}],"source":["!hdfs dfs -ls wordcount/output"]},{"cell_type":"markdown","metadata":{"id":"qSVTe1_jD3FR"},"source":["If `output` contains a file named `_SUCCESS` that means that the mapreduce job completed successfully.\n","\n","**Note:** when dealing with Big Data it's always advisable to pipe the output of `cat` commands to `head` (or `tail`)."]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aOiDVpQDD3FR","executionInfo":{"status":"ok","timestamp":1713895269081,"user_tz":-420,"elapsed":5558,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}},"outputId":"a08d3395-962b-4b21-d0b3-21c533c11a24"},"outputs":[{"output_type":"stream","name":"stdout","text":["apple\t1\n","apple\t1\n","banana\t1\n","orange\t1\n","peach\t1\n","peach\t1\n","peach\t1\n","peach\t1\n","pineapple\t1\n"]}],"source":["!hdfs dfs -cat wordcount/output/part*|head"]},{"cell_type":"markdown","metadata":{"id":"QOpW1oAcD3FS"},"source":["We have gotten as expected all the output from the mapper. Something worth of notice is that the data outputted from the mapper _**has been sorted**_. We haven't asked for that but this step is automatically performed by the mapper as soon as the number of reducers is $\\gt 0$."]},{"cell_type":"markdown","metadata":{"id":"zJFVuyTAD3FS"},"source":["### Shuffling and sorting <a name=\"shuffling&sorting\"></a>\n","The following picture illustrates the concept of shuffling and sorting that is automatically performed by Hadoop after each map before passing the output to reduce. In the picture the outputs of the two mapper tasks are shown. The arrows represent shuffling and sorting done before delivering the data to one reducer (rightmost box).\n","![Shuffle & sort](shuffle_sort.png)\n","The shuffling and sorting phase is often one of the most costly in a MapReduce job.\n","\n","\n","<b>Note:</b> the job ran with two mappers because $2$ is the default number of mappers in Hadoop."]},{"cell_type":"markdown","metadata":{"id":"2OLEKdNMD3FS"},"source":["## The reducer <a name=\"reducer\"></a>\n","Let's now write a reducer script called `reduce.sh`."]},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GeXiCVmDD3FS","executionInfo":{"status":"ok","timestamp":1713895269082,"user_tz":-420,"elapsed":28,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}},"outputId":"498e802d-d9ab-4a6a-b785-ad0dd7a3a5ad"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing reduce.sh\n"]}],"source":["%%writefile reduce.sh\n","#!/bin/bash\n","\n","currkey=\"\"\n","currcount=0\n","while IFS=$'\\t' read -r key val\n","do\n","  if [[ $key == $currkey ]]\n","  then\n","      currcount=$(( currcount + val ))\n","  else\n","    if [ -n \"$currkey\" ]\n","    then\n","      echo -e ${currkey} \"\\t\" ${currcount}\n","    fi\n","    currkey=$key\n","    currcount=1\n","  fi\n","done\n","# last one\n","echo -e ${currkey} \"\\t\" ${currcount}"]},{"cell_type":"markdown","metadata":{"id":"_inmhPDGD3FS"},"source":["Set permission for the reducer script."]},{"cell_type":"code","execution_count":41,"metadata":{"id":"fqA0Ag2yD3FS","executionInfo":{"status":"ok","timestamp":1713895269082,"user_tz":-420,"elapsed":26,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}}},"outputs":[],"source":["!chmod 700 reduce.sh"]},{"cell_type":"markdown","metadata":{"id":"SaD2UV0QD3FT"},"source":["### Test and run <a name=\"run\"></a>\n","\n","Test map and reduce on the shell"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"49VknYKbD3FU","executionInfo":{"status":"ok","timestamp":1713895269082,"user_tz":-420,"elapsed":25,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}},"outputId":"d66e2b51-e551-4b26-9c11-519032e732a2"},"outputs":[{"output_type":"stream","name":"stdout","text":["apple \t 2\n","banana \t 1\n","orange \t 1\n","peach \t 4\n","pineapple \t 1\n"]}],"source":["!cat fruits.txt|./map.sh|sort|./reduce.sh"]},{"cell_type":"markdown","metadata":{"id":"m05eooyDD3FU"},"source":["Once we've made sure that the reducer script runs correctly on the shell, we can run it on the cluster."]},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qX_In5JID3FU","executionInfo":{"status":"ok","timestamp":1713895275368,"user_tz":-420,"elapsed":6309,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}},"outputId":"fa3dfdbb-6ebb-4cc1-ce31-f01b44da31e6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Deleted wordcount/output\n","packageJobJar: [map.sh, reduce.sh] [] /tmp/streamjob13404765489269544221.jar tmpDir=null\n"]},{"output_type":"stream","name":"stderr","text":["2024-04-23 18:01:12,377 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n","2024-04-23 18:01:13,379 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n","2024-04-23 18:01:13,589 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n","2024-04-23 18:01:13,590 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n","2024-04-23 18:01:13,629 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2024-04-23 18:01:13,910 INFO mapred.FileInputFormat: Total input files to process : 1\n","2024-04-23 18:01:13,959 INFO mapreduce.JobSubmitter: number of splits:1\n","2024-04-23 18:01:14,313 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1832925837_0001\n","2024-04-23 18:01:14,313 INFO mapreduce.JobSubmitter: Executing with tokens: []\n","2024-04-23 18:01:14,898 INFO mapred.LocalDistributedCacheManager: Localized file:/content/map.sh as file:/tmp/hadoop-root/mapred/local/job_local1832925837_0001_d4ee6e6e-c47c-4f72-ad60-1f1bf07a92e5/map.sh\n","2024-04-23 18:01:14,933 INFO mapred.LocalDistributedCacheManager: Localized file:/content/reduce.sh as file:/tmp/hadoop-root/mapred/local/job_local1832925837_0001_0f797fba-bb79-4686-8dfb-0a304c5ebcec/reduce.sh\n","2024-04-23 18:01:15,060 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n","2024-04-23 18:01:15,063 INFO mapreduce.Job: Running job: job_local1832925837_0001\n","2024-04-23 18:01:15,071 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n","2024-04-23 18:01:15,075 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n","2024-04-23 18:01:15,093 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-04-23 18:01:15,093 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-04-23 18:01:15,160 INFO mapred.LocalJobRunner: Waiting for map tasks\n","2024-04-23 18:01:15,169 INFO mapred.LocalJobRunner: Starting task: attempt_local1832925837_0001_m_000000_0\n","2024-04-23 18:01:15,228 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-04-23 18:01:15,229 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-04-23 18:01:15,260 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2024-04-23 18:01:15,275 INFO mapred.MapTask: Processing split: file:/content/wordcount/input/fruits.txt:0+60\n","2024-04-23 18:01:15,312 INFO mapred.MapTask: numReduceTasks: 1\n","2024-04-23 18:01:15,437 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2024-04-23 18:01:15,437 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2024-04-23 18:01:15,437 INFO mapred.MapTask: soft limit at 83886080\n","2024-04-23 18:01:15,437 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2024-04-23 18:01:15,437 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2024-04-23 18:01:15,443 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2024-04-23 18:01:15,452 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./map.sh]\n","2024-04-23 18:01:15,458 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n","2024-04-23 18:01:15,461 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n","2024-04-23 18:01:15,462 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n","2024-04-23 18:01:15,463 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n","2024-04-23 18:01:15,464 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n","2024-04-23 18:01:15,464 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n","2024-04-23 18:01:15,466 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n","2024-04-23 18:01:15,467 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n","2024-04-23 18:01:15,467 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n","2024-04-23 18:01:15,468 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n","2024-04-23 18:01:15,469 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n","2024-04-23 18:01:15,469 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n","2024-04-23 18:01:15,497 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2024-04-23 18:01:15,499 INFO streaming.PipeMapRed: MRErrorThread done\n","2024-04-23 18:01:15,502 INFO streaming.PipeMapRed: Records R/W=3/1\n","2024-04-23 18:01:15,502 INFO streaming.PipeMapRed: mapRedFinished\n","2024-04-23 18:01:15,506 INFO mapred.LocalJobRunner: \n","2024-04-23 18:01:15,506 INFO mapred.MapTask: Starting flush of map output\n","2024-04-23 18:01:15,506 INFO mapred.MapTask: Spilling map output\n","2024-04-23 18:01:15,506 INFO mapred.MapTask: bufstart = 0; bufend = 78; bufvoid = 104857600\n","2024-04-23 18:01:15,506 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214364(104857456); length = 33/6553600\n","2024-04-23 18:01:15,516 INFO mapred.MapTask: Finished spill 0\n","2024-04-23 18:01:15,533 INFO mapred.Task: Task:attempt_local1832925837_0001_m_000000_0 is done. And is in the process of committing\n","2024-04-23 18:01:15,536 INFO mapred.LocalJobRunner: Records R/W=3/1\n","2024-04-23 18:01:15,537 INFO mapred.Task: Task 'attempt_local1832925837_0001_m_000000_0' done.\n","2024-04-23 18:01:15,546 INFO mapred.Task: Final Counters for attempt_local1832925837_0001_m_000000_0: Counters: 17\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=1200\n","\t\tFILE: Number of bytes written=720037\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=3\n","\t\tMap output records=9\n","\t\tMap output bytes=78\n","\t\tMap output materialized bytes=102\n","\t\tInput split bytes=92\n","\t\tCombine input records=0\n","\t\tSpilled Records=9\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=29\n","\t\tTotal committed heap usage (bytes)=399507456\n","\tFile Input Format Counters \n","\t\tBytes Read=76\n","2024-04-23 18:01:15,546 INFO mapred.LocalJobRunner: Finishing task: attempt_local1832925837_0001_m_000000_0\n","2024-04-23 18:01:15,546 INFO mapred.LocalJobRunner: map task executor complete.\n","2024-04-23 18:01:15,550 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n","2024-04-23 18:01:15,551 INFO mapred.LocalJobRunner: Starting task: attempt_local1832925837_0001_r_000000_0\n","2024-04-23 18:01:15,562 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-04-23 18:01:15,562 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-04-23 18:01:15,563 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2024-04-23 18:01:15,570 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2a204da4\n","2024-04-23 18:01:15,573 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2024-04-23 18:01:15,625 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n","2024-04-23 18:01:15,630 INFO reduce.EventFetcher: attempt_local1832925837_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n","2024-04-23 18:01:15,701 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1832925837_0001_m_000000_0 decomp: 98 len: 102 to MEMORY\n","2024-04-23 18:01:15,707 INFO reduce.InMemoryMapOutput: Read 98 bytes from map-output for attempt_local1832925837_0001_m_000000_0\n","2024-04-23 18:01:15,709 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 98, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->98\n","2024-04-23 18:01:15,712 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n","2024-04-23 18:01:15,714 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-04-23 18:01:15,714 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n","2024-04-23 18:01:15,723 INFO mapred.Merger: Merging 1 sorted segments\n","2024-04-23 18:01:15,723 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 90 bytes\n","2024-04-23 18:01:15,725 INFO reduce.MergeManagerImpl: Merged 1 segments, 98 bytes to disk to satisfy reduce memory limit\n","2024-04-23 18:01:15,725 INFO reduce.MergeManagerImpl: Merging 1 files, 102 bytes from disk\n","2024-04-23 18:01:15,726 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n","2024-04-23 18:01:15,726 INFO mapred.Merger: Merging 1 sorted segments\n","2024-04-23 18:01:15,727 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 90 bytes\n","2024-04-23 18:01:15,728 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-04-23 18:01:15,740 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./reduce.sh]\n","2024-04-23 18:01:15,744 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n","2024-04-23 18:01:15,748 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n","2024-04-23 18:01:15,765 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2024-04-23 18:01:15,769 INFO streaming.PipeMapRed: Records R/W=9/1\n","2024-04-23 18:01:15,770 INFO streaming.PipeMapRed: MRErrorThread done\n","2024-04-23 18:01:15,770 INFO streaming.PipeMapRed: mapRedFinished\n","2024-04-23 18:01:15,772 INFO mapred.Task: Task:attempt_local1832925837_0001_r_000000_0 is done. And is in the process of committing\n","2024-04-23 18:01:15,774 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-04-23 18:01:15,774 INFO mapred.Task: Task attempt_local1832925837_0001_r_000000_0 is allowed to commit now\n","2024-04-23 18:01:15,776 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1832925837_0001_r_000000_0' to file:/content/wordcount/output\n","2024-04-23 18:01:15,777 INFO mapred.LocalJobRunner: Records R/W=9/1 > reduce\n","2024-04-23 18:01:15,778 INFO mapred.Task: Task 'attempt_local1832925837_0001_r_000000_0' done.\n","2024-04-23 18:01:15,778 INFO mapred.Task: Final Counters for attempt_local1832925837_0001_r_000000_0: Counters: 24\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=1436\n","\t\tFILE: Number of bytes written=720207\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=5\n","\t\tReduce shuffle bytes=102\n","\t\tReduce input records=9\n","\t\tReduce output records=5\n","\t\tSpilled Records=9\n","\t\tShuffled Maps =1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=1\n","\t\tGC time elapsed (ms)=14\n","\t\tTotal committed heap usage (bytes)=399507456\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Output Format Counters \n","\t\tBytes Written=68\n","2024-04-23 18:01:15,778 INFO mapred.LocalJobRunner: Finishing task: attempt_local1832925837_0001_r_000000_0\n","2024-04-23 18:01:15,779 INFO mapred.LocalJobRunner: reduce task executor complete.\n","2024-04-23 18:01:16,070 INFO mapreduce.Job: Job job_local1832925837_0001 running in uber mode : false\n","2024-04-23 18:01:16,071 INFO mapreduce.Job:  map 100% reduce 100%\n","2024-04-23 18:01:16,072 INFO mapreduce.Job: Job job_local1832925837_0001 completed successfully\n","2024-04-23 18:01:16,083 INFO mapreduce.Job: Counters: 30\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=2636\n","\t\tFILE: Number of bytes written=1440244\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=3\n","\t\tMap output records=9\n","\t\tMap output bytes=78\n","\t\tMap output materialized bytes=102\n","\t\tInput split bytes=92\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=5\n","\t\tReduce shuffle bytes=102\n","\t\tReduce input records=9\n","\t\tReduce output records=5\n","\t\tSpilled Records=18\n","\t\tShuffled Maps =1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=1\n","\t\tGC time elapsed (ms)=43\n","\t\tTotal committed heap usage (bytes)=799014912\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Input Format Counters \n","\t\tBytes Read=76\n","\tFile Output Format Counters \n","\t\tBytes Written=68\n","2024-04-23 18:01:16,083 INFO streaming.StreamJob: Output directory: wordcount/output\n"]}],"source":["%%bash\n","hdfs dfs -rm -r wordcount/output 2>/dev/null\n","mapred streaming \\\n","  -file map.sh \\\n","  -file reduce.sh \\\n","  -input wordcount/input \\\n","  -output wordcount/output \\\n","  -mapper map.sh \\\n","  -reducer reduce.sh"]},{"cell_type":"markdown","metadata":{"id":"k4rVxTFiD3FV"},"source":["Let's check the output on the HDFS filesystem"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XFa3o02jD3FV","executionInfo":{"status":"ok","timestamp":1713895277970,"user_tz":-420,"elapsed":2613,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}},"outputId":"7c589b12-13a5-47ca-a724-df6c6a45d3cd"},"outputs":[{"output_type":"stream","name":"stdout","text":["apple \t 2\n","banana \t 1\n","orange \t 1\n","peach \t 4\n","pineapple \t 1\n"]}],"source":["!hdfs dfs -cat wordcount/output/part*|head"]},{"cell_type":"markdown","metadata":{"id":"COUcf-XOD3FW"},"source":["## Run a mapreduce job with more data <a name=\"moredata\"></a>\n","\n","Let's create a datafile by downloading some real data, for instance from a Web page. This example will be used to introduce some advanced configurations.\n","\n","Next, we download a URL with `wget` and filter out HTML tags with a `sed` regular expression."]},{"cell_type":"code","execution_count":45,"metadata":{"id":"MdvWEhzBD3FW","executionInfo":{"status":"ok","timestamp":1713895278356,"user_tz":-420,"elapsed":396,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}}},"outputs":[],"source":["%%bash\n","URL=https://www.derstandard.at/story/2000110819049/und-wo-warst-du-beim-fall-der-mauer\n","wget -qO- $URL | sed -e 's/<[^>]*>//g;s/^ //g' >sample_article.txt"]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fmW-whJ6D3FW","executionInfo":{"status":"ok","timestamp":1713895278356,"user_tz":-420,"elapsed":11,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}},"outputId":"3bcd1061-db2b-4c16-fa24-1ba42eef92a0"},"outputs":[{"output_type":"stream","name":"stdout","text":["\r\t1\n","\r\t1\n","\r\t1\n","\r\t1\n","window.DERSTANDARD.pageConfig.init({\"edition\":\"at\",\"environment\":\"Production\",\"baseUrls\":{\"currentDocument\":\"https://www.derstandard.at\",\"authorization\":\"https://apps.derstandard.at/autorisierung\",\"userprofile\":\"https://apps.derstandard.at/userprofil\",\"staticfiles\":\"https://at.staticfiles.at\"},\"settings\":{\"disableNotifications\":false}})\r\t1\n","\r\t1\n","\r\t1\n","\r\t1\n","Und\t1\n","wo\t1\n"]}],"source":["!cat sample_article.txt|./map.sh|head"]},{"cell_type":"markdown","metadata":{"id":"uOaIENfWD3FX"},"source":["As usual, with real data there's some more work to do. Here we see that the mapper script doesn't skip empty lines. Let's modify it so that empty lines are skipped."]},{"cell_type":"code","execution_count":47,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uljg1_teD3FX","executionInfo":{"status":"ok","timestamp":1713895278356,"user_tz":-420,"elapsed":6,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}},"outputId":"b7754184-9240-4b3c-b369-2834ad7b1051"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting map.sh\n"]}],"source":["%%writefile map.sh\n","#!/bin/bash\n","\n","while read line\n","do\n"," for word in $line\n"," do\n","  if [[ \"$line\" =~ [^[:space:]] ]]\n","  then\n","    if [ -n \"$word\" ]\n","    then\n","    echo -e ${word} \"\\t1\"\n","    fi\n","  fi\n"," done\n","done"]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bnp1fwzxD3FY","executionInfo":{"status":"ok","timestamp":1713895279026,"user_tz":-420,"elapsed":673,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}},"outputId":"1925e1b5-7f49-4950-b7ba-66f9da4ebfcf"},"outputs":[{"output_type":"stream","name":"stdout","text":["window.DERSTANDARD.pageConfig.init({\"edition\":\"at\",\"environment\":\"Production\",\"baseUrls\":{\"currentDocument\":\"https://www.derstandard.at\",\"authorization\":\"https://apps.derstandard.at/autorisierung\",\"userprofile\":\"https://apps.derstandard.at/userprofil\",\"staticfiles\":\"https://at.staticfiles.at\"},\"settings\":{\"disableNotifications\":false}})\r \t1\n","Und \t1\n","wo \t1\n","warst \t1\n","du \t1\n","beim \t1\n","Fall \t1\n","der \t1\n","Mauer? \t1\n","- \t1\n"]}],"source":["!cat sample_article.txt|./map.sh|head"]},{"cell_type":"markdown","metadata":{"id":"xvb0h_hDD3FY"},"source":["Now the output of `map.sh` looks better!\n","\n","<b>Note:</b> when working with real data we need in general some more preprocessing in order to remove control characters or invalid unicode.\n","\n","Time to run MapReduce again with the new data, but first we need to \"put\" the data on HDFS."]},{"cell_type":"code","execution_count":49,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sB6QXbf0D3FY","executionInfo":{"status":"ok","timestamp":1713895283835,"user_tz":-420,"elapsed":4811,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}},"outputId":"68ac3b0c-a296-42bc-97db-a7edeccfbebd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Deleted wordcount/input\n"]}],"source":["%%bash\n","hdfs dfs -rm -r wordcount/input 2>/dev/null\n","hdfs dfs -put sample_article.txt wordcount/input"]},{"cell_type":"code","execution_count":50,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lk9bvaioD3FZ","executionInfo":{"status":"ok","timestamp":1713895286073,"user_tz":-420,"elapsed":2249,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}},"outputId":"e9a94aea-2077-4d4f-ea06-a328b3426560"},"outputs":[{"output_type":"stream","name":"stdout","text":["-rw-r--r--   1 root root     29.0 K 2024-04-23 18:01 wordcount/input\n"]}],"source":["# check that the folder wordcount/input on HDFS only contains sample_article.txt\n","!hdfs dfs -ls -h wordcount/input"]},{"cell_type":"markdown","metadata":{"id":"5XvAhcSuD3Fa"},"source":["Check the reducer"]},{"cell_type":"code","execution_count":51,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2oYQDdc7D3Fa","executionInfo":{"status":"ok","timestamp":1713895286073,"user_tz":-420,"elapsed":5,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}},"outputId":"ca06abb0-65a1-4bc0-9530-7cd8cb90a608"},"outputs":[{"output_type":"stream","name":"stdout","text":["window.DERSTANDARD.pageConfig.init({\"edition\":\"at\",\"environment\":\"Production\",\"baseUrls\":{\"currentDocument\":\"https://www.derstandard.at\",\"authorization\":\"https://apps.derstandard.at/autorisierung\",\"userprofile\":\"https://apps.derstandard.at/userprofil\",\"staticfiles\":\"https://at.staticfiles.at\"},\"settings\":{\"disableNotifications\":false}})\r \t 1\n","Und \t 1\n","wo \t 1\n","warst \t 1\n","du \t 1\n","beim \t 1\n","Fall \t 1\n","der \t 1\n","Mauer? \t 1\n","- \t 1\n"]}],"source":["!cat sample_article.txt|./map.sh|./reduce.sh|head"]},{"cell_type":"code","execution_count":52,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m_xSCtZ0D3Fa","executionInfo":{"status":"ok","timestamp":1713895295248,"user_tz":-420,"elapsed":9177,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}},"outputId":"ec5f45e0-fe87-4445-d5d1-4edf427ca584"},"outputs":[{"output_type":"stream","name":"stdout","text":["Deleted wordcount/output\n","packageJobJar: [map.sh, reduce.sh] [] /tmp/streamjob6730509373929717116.jar tmpDir=null\n"]},{"output_type":"stream","name":"stderr","text":["2024-04-23 18:01:30,507 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n","2024-04-23 18:01:32,037 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n","2024-04-23 18:01:32,417 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n","2024-04-23 18:01:32,417 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n","2024-04-23 18:01:32,479 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2024-04-23 18:01:32,916 INFO mapred.FileInputFormat: Total input files to process : 1\n","2024-04-23 18:01:32,985 INFO mapreduce.JobSubmitter: number of splits:1\n","2024-04-23 18:01:33,585 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1632254583_0001\n","2024-04-23 18:01:33,586 INFO mapreduce.JobSubmitter: Executing with tokens: []\n","2024-04-23 18:01:34,344 INFO mapred.LocalDistributedCacheManager: Localized file:/content/map.sh as file:/tmp/hadoop-root/mapred/local/job_local1632254583_0001_bb4651a3-7dd0-463b-80b1-d68773f3420c/map.sh\n","2024-04-23 18:01:34,379 INFO mapred.LocalDistributedCacheManager: Localized file:/content/reduce.sh as file:/tmp/hadoop-root/mapred/local/job_local1632254583_0001_da9660d1-1a2f-4ea8-9ab5-1818625dd6f0/reduce.sh\n","2024-04-23 18:01:34,528 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n","2024-04-23 18:01:34,530 INFO mapreduce.Job: Running job: job_local1632254583_0001\n","2024-04-23 18:01:34,540 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n","2024-04-23 18:01:34,543 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n","2024-04-23 18:01:34,549 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-04-23 18:01:34,549 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-04-23 18:01:34,615 INFO mapred.LocalJobRunner: Waiting for map tasks\n","2024-04-23 18:01:34,620 INFO mapred.LocalJobRunner: Starting task: attempt_local1632254583_0001_m_000000_0\n","2024-04-23 18:01:34,670 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-04-23 18:01:34,674 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-04-23 18:01:34,719 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2024-04-23 18:01:34,735 INFO mapred.MapTask: Processing split: file:/content/wordcount/input:0+29720\n","2024-04-23 18:01:34,754 INFO mapred.MapTask: numReduceTasks: 1\n","2024-04-23 18:01:34,873 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2024-04-23 18:01:34,873 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2024-04-23 18:01:34,873 INFO mapred.MapTask: soft limit at 83886080\n","2024-04-23 18:01:34,873 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2024-04-23 18:01:34,873 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2024-04-23 18:01:34,878 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2024-04-23 18:01:34,886 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./map.sh]\n","2024-04-23 18:01:34,895 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n","2024-04-23 18:01:34,901 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n","2024-04-23 18:01:34,902 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n","2024-04-23 18:01:34,902 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n","2024-04-23 18:01:34,903 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n","2024-04-23 18:01:34,903 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n","2024-04-23 18:01:34,905 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n","2024-04-23 18:01:34,906 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n","2024-04-23 18:01:34,906 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n","2024-04-23 18:01:34,907 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n","2024-04-23 18:01:34,908 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n","2024-04-23 18:01:34,909 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n","2024-04-23 18:01:34,943 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2024-04-23 18:01:34,943 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2024-04-23 18:01:34,945 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2024-04-23 18:01:34,954 INFO streaming.PipeMapRed: Records R/W=186/1\n","2024-04-23 18:01:35,255 INFO streaming.PipeMapRed: MRErrorThread done\n","2024-04-23 18:01:35,256 INFO streaming.PipeMapRed: mapRedFinished\n","2024-04-23 18:01:35,261 INFO mapred.LocalJobRunner: \n","2024-04-23 18:01:35,261 INFO mapred.MapTask: Starting flush of map output\n","2024-04-23 18:01:35,261 INFO mapred.MapTask: Spilling map output\n","2024-04-23 18:01:35,261 INFO mapred.MapTask: bufstart = 0; bufend = 32527; bufvoid = 104857600\n","2024-04-23 18:01:35,261 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26209816(104839264); length = 4581/6553600\n","2024-04-23 18:01:35,296 INFO mapred.MapTask: Finished spill 0\n","2024-04-23 18:01:35,325 INFO mapred.Task: Task:attempt_local1632254583_0001_m_000000_0 is done. And is in the process of committing\n","2024-04-23 18:01:35,329 INFO mapred.LocalJobRunner: Records R/W=186/1\n","2024-04-23 18:01:35,330 INFO mapred.Task: Task 'attempt_local1632254583_0001_m_000000_0' done.\n","2024-04-23 18:01:35,358 INFO mapred.Task: Final Counters for attempt_local1632254583_0001_m_000000_0: Counters: 17\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=31161\n","\t\tFILE: Number of bytes written=754875\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=186\n","\t\tMap output records=1146\n","\t\tMap output bytes=32527\n","\t\tMap output materialized bytes=34873\n","\t\tInput split bytes=81\n","\t\tCombine input records=0\n","\t\tSpilled Records=1146\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=425721856\n","\tFile Input Format Counters \n","\t\tBytes Read=29968\n","2024-04-23 18:01:35,366 INFO mapred.LocalJobRunner: Finishing task: attempt_local1632254583_0001_m_000000_0\n","2024-04-23 18:01:35,366 INFO mapred.LocalJobRunner: map task executor complete.\n","2024-04-23 18:01:35,380 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n","2024-04-23 18:01:35,381 INFO mapred.LocalJobRunner: Starting task: attempt_local1632254583_0001_r_000000_0\n","2024-04-23 18:01:35,421 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-04-23 18:01:35,421 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-04-23 18:01:35,422 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2024-04-23 18:01:35,440 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@550948bc\n","2024-04-23 18:01:35,444 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2024-04-23 18:01:35,480 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n","2024-04-23 18:01:35,494 INFO reduce.EventFetcher: attempt_local1632254583_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n","2024-04-23 18:01:35,539 INFO mapreduce.Job: Job job_local1632254583_0001 running in uber mode : false\n","2024-04-23 18:01:35,543 INFO mapreduce.Job:  map 100% reduce 0%\n","2024-04-23 18:01:35,551 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1632254583_0001_m_000000_0 decomp: 34869 len: 34873 to MEMORY\n","2024-04-23 18:01:35,558 INFO reduce.InMemoryMapOutput: Read 34869 bytes from map-output for attempt_local1632254583_0001_m_000000_0\n","2024-04-23 18:01:35,564 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 34869, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->34869\n","2024-04-23 18:01:35,567 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n","2024-04-23 18:01:35,569 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-04-23 18:01:35,569 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n","2024-04-23 18:01:35,578 INFO mapred.Merger: Merging 1 sorted segments\n","2024-04-23 18:01:35,578 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 34851 bytes\n","2024-04-23 18:01:35,589 INFO reduce.MergeManagerImpl: Merged 1 segments, 34869 bytes to disk to satisfy reduce memory limit\n","2024-04-23 18:01:35,590 INFO reduce.MergeManagerImpl: Merging 1 files, 34873 bytes from disk\n","2024-04-23 18:01:35,592 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n","2024-04-23 18:01:35,592 INFO mapred.Merger: Merging 1 sorted segments\n","2024-04-23 18:01:35,593 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 34851 bytes\n","2024-04-23 18:01:35,594 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-04-23 18:01:35,616 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./reduce.sh]\n","2024-04-23 18:01:35,621 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n","2024-04-23 18:01:35,625 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n","2024-04-23 18:01:35,653 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2024-04-23 18:01:35,653 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2024-04-23 18:01:35,655 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2024-04-23 18:01:35,667 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2024-04-23 18:01:35,672 INFO streaming.PipeMapRed: Records R/W=1146/1\n","2024-04-23 18:01:35,754 INFO streaming.PipeMapRed: MRErrorThread done\n","2024-04-23 18:01:35,755 INFO streaming.PipeMapRed: mapRedFinished\n","2024-04-23 18:01:35,758 INFO mapred.Task: Task:attempt_local1632254583_0001_r_000000_0 is done. And is in the process of committing\n","2024-04-23 18:01:35,759 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-04-23 18:01:35,759 INFO mapred.Task: Task attempt_local1632254583_0001_r_000000_0 is allowed to commit now\n","2024-04-23 18:01:35,762 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1632254583_0001_r_000000_0' to file:/content/wordcount/output\n","2024-04-23 18:01:35,764 INFO mapred.LocalJobRunner: Records R/W=1146/1 > reduce\n","2024-04-23 18:01:35,765 INFO mapred.Task: Task 'attempt_local1632254583_0001_r_000000_0' done.\n","2024-04-23 18:01:35,766 INFO mapred.Task: Final Counters for attempt_local1632254583_0001_r_000000_0: Counters: 24\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=100939\n","\t\tFILE: Number of bytes written=819340\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=736\n","\t\tReduce shuffle bytes=34873\n","\t\tReduce input records=1146\n","\t\tReduce output records=746\n","\t\tSpilled Records=1146\n","\t\tShuffled Maps =1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=1\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=425721856\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Output Format Counters \n","\t\tBytes Written=29592\n","2024-04-23 18:01:35,768 INFO mapred.LocalJobRunner: Finishing task: attempt_local1632254583_0001_r_000000_0\n","2024-04-23 18:01:35,768 INFO mapred.LocalJobRunner: reduce task executor complete.\n","2024-04-23 18:01:36,545 INFO mapreduce.Job:  map 100% reduce 100%\n","2024-04-23 18:01:36,546 INFO mapreduce.Job: Job job_local1632254583_0001 completed successfully\n","2024-04-23 18:01:36,557 INFO mapreduce.Job: Counters: 30\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=132100\n","\t\tFILE: Number of bytes written=1574215\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=186\n","\t\tMap output records=1146\n","\t\tMap output bytes=32527\n","\t\tMap output materialized bytes=34873\n","\t\tInput split bytes=81\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=736\n","\t\tReduce shuffle bytes=34873\n","\t\tReduce input records=1146\n","\t\tReduce output records=746\n","\t\tSpilled Records=2292\n","\t\tShuffled Maps =1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=1\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=851443712\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Input Format Counters \n","\t\tBytes Read=29968\n","\tFile Output Format Counters \n","\t\tBytes Written=29592\n","2024-04-23 18:01:36,557 INFO streaming.StreamJob: Output directory: wordcount/output\n"]}],"source":["%%bash\n","hadoop fs -rmr wordcount/output 2>/dev/null\n","mapred streaming \\\n","  -file map.sh \\\n","  -file reduce.sh \\\n","  -input wordcount/input \\\n","  -output wordcount/output \\\n","  -mapper map.sh \\\n","  -reducer reduce.sh"]},{"cell_type":"markdown","metadata":{"id":"hWl5JZ8wD3Fb"},"source":["Check the output on HDFS"]},{"cell_type":"code","execution_count":53,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uh0m4Zp8D3Fb","executionInfo":{"status":"ok","timestamp":1713895297665,"user_tz":-420,"elapsed":2423,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}},"outputId":"9976940e-a092-4774-ff08-76602b8a526a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 2 items\n","-rw-r--r--   1 root root          0 2024-04-23 18:01 wordcount/output/_SUCCESS\n","-rw-r--r--   1 root root      29352 2024-04-23 18:01 wordcount/output/part-00000\n"]}],"source":["!hdfs dfs -ls wordcount/output"]},{"cell_type":"markdown","metadata":{"id":"70a5eU1pD3Fb"},"source":["This job took a few seconds and this is quite some time for such a small file (4KB). This is due to the overhead of distributing the data and running the Hadoop framework.\n","The advantage of Hadoop can be appreciated only for large datasets."]},{"cell_type":"code","execution_count":54,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5ls0Ui7hD3Fb","executionInfo":{"status":"ok","timestamp":1713895300318,"user_tz":-420,"elapsed":2658,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}},"outputId":"469bea55-4ad5-42d5-e739-ab5942013515"},"outputs":[{"output_type":"stream","name":"stdout","text":["!!n.frames[t]; \t 1\n","!0)); \t 1\n","!1)) \t 1\n","!1, \t 1\n","!= \t 1\n","!== \t 2\n","!function \t 2\n","!r \t 1\n","\"'+n+'\"',o)}return{key:r,value:e.substr(t+1)}},t._renewCache=function(){t._cache=t._getCacheFromString(t._document.cookie),t._cachedDocumentCookie=t._document.cookie},t._areEnabled=function(){var \t 1\n","\"))}function \t 1\n","cat: Unable to write to output stream.\n"]}],"source":["!hdfs dfs -cat wordcount/output/part-00000|head"]},{"cell_type":"markdown","metadata":{"id":"Wvt3S7uWD3Fb"},"source":["### Sort the output with `sort` <a name=\"sortoutput\"></a>\n","\n","We've obtained a list of tokens that appear in the file followed by their frequencies.\n","\n","The output of the reducer is sorted by key (the word) because that's the ordering that the reducer becomes from the mapper. If we're interested in sorting the data by frequency, we can use the Unix `sort` command (with the options `k2`, `n`, `r` respectively \"by field 2\", \"numeric\", \"reverse\")."]},{"cell_type":"code","execution_count":55,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vBq1abi8D3Fc","executionInfo":{"status":"ok","timestamp":1713895302641,"user_tz":-420,"elapsed":2325,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}},"outputId":"49669828-847a-4647-bd55-9bde874a2eb1"},"outputs":[{"output_type":"stream","name":"stdout","text":["= \t 40\n","{ \t 22\n","var \t 22\n","&& \t 19\n","strict\";function \t 13\n","} \t 12\n","in \t 12\n","not \t 12\n","to \t 10\n","e&&e.__esModule?e:{\"default\":e}}function \t 9\n"]}],"source":["!hdfs dfs -cat wordcount/output/part-00000|sort -k2nr|head"]},{"cell_type":"markdown","metadata":{"id":"M89ZZizrD3Fc"},"source":["The most common word appears to be \"die\" (the German for the definite article \"the\")."]},{"cell_type":"markdown","metadata":{"id":"RyAIHjQdD3Fc"},"source":["### Sort the output with another MapReduce job <a name=\"sortoutputMR\"></a>\n","\n","If we wanted to sort the output of the reducer using the mapreduce framework, we could employ a simple trick: create a mapper that interchanges words with their frequency values. Since by construction mappers sort their output by key, we get the desired sorting as a side-effect.\n","\n","Call the new mapper `swap_keyval.sh`."]},{"cell_type":"code","execution_count":56,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xVoMDI8MD3Fc","executionInfo":{"status":"ok","timestamp":1713895302641,"user_tz":-420,"elapsed":4,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}},"outputId":"15b3b47d-beb9-4ac0-f7c6-0f06bf6b7742"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing swap_keyval.sh\n"]}],"source":["%%writefile swap_keyval.sh\n","#!/bin/bash\n","# This script will read one line at a time and swap key/value\n","# For instance, the line \"word 100\" will become \"100 word\"\n","\n","while read key val\n","do\n"," printf \"%s\\t%s\\n\" \"$val\" \"$key\"\n","done"]},{"cell_type":"markdown","metadata":{"id":"oLkYiRvHD3Fd"},"source":["We are going to run the swap mapper script on the output of the previous mapreduce job. Note that in the below cell we are not deleting the previous output but instead we're saving the output from the current job in a new folder `output_sorted`.\n","\n","Nice thing about running a job on the output of a preceding job is that we do not need to upload files to HDFS because the data is already on HDFS. Not so nice: writing data to disk at each step of a data transformation pipeline takes time and this can be costly for longer data pipelines. This is one of the shortcomings of MapReduce that are addressed by [Apache Spark](https://spark.apache.org/)."]},{"cell_type":"code","execution_count":57,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qhpbRr5HD3Fd","executionInfo":{"status":"ok","timestamp":1713895311643,"user_tz":-420,"elapsed":9004,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}},"outputId":"b5348971-5ac4-467f-d071-ab4bf4a9ae7c"},"outputs":[{"output_type":"stream","name":"stdout","text":["packageJobJar: [swap_keyval.sh] [] /tmp/streamjob659543932481634598.jar tmpDir=null\n"]},{"output_type":"stream","name":"stderr","text":["2024-04-23 18:01:48,699 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n","2024-04-23 18:01:49,551 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n","2024-04-23 18:01:49,753 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n","2024-04-23 18:01:49,753 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n","2024-04-23 18:01:49,795 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2024-04-23 18:01:50,043 INFO mapred.FileInputFormat: Total input files to process : 1\n","2024-04-23 18:01:50,067 INFO mapreduce.JobSubmitter: number of splits:1\n","2024-04-23 18:01:50,412 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local61580702_0001\n","2024-04-23 18:01:50,413 INFO mapreduce.JobSubmitter: Executing with tokens: []\n","2024-04-23 18:01:51,054 INFO mapred.LocalDistributedCacheManager: Localized file:/content/swap_keyval.sh as file:/tmp/hadoop-root/mapred/local/job_local61580702_0001_775e6ad0-22d2-4e5a-9b2b-d505cce99ca2/swap_keyval.sh\n","2024-04-23 18:01:51,187 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n","2024-04-23 18:01:51,190 INFO mapreduce.Job: Running job: job_local61580702_0001\n","2024-04-23 18:01:51,199 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n","2024-04-23 18:01:51,202 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n","2024-04-23 18:01:51,212 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-04-23 18:01:51,212 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-04-23 18:01:51,288 INFO mapred.LocalJobRunner: Waiting for map tasks\n","2024-04-23 18:01:51,294 INFO mapred.LocalJobRunner: Starting task: attempt_local61580702_0001_m_000000_0\n","2024-04-23 18:01:51,338 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-04-23 18:01:51,341 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-04-23 18:01:51,383 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2024-04-23 18:01:51,397 INFO mapred.MapTask: Processing split: file:/content/wordcount/output/part-00000:0+29352\n","2024-04-23 18:01:51,422 INFO mapred.MapTask: numReduceTasks: 1\n","2024-04-23 18:01:51,542 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2024-04-23 18:01:51,542 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2024-04-23 18:01:51,542 INFO mapred.MapTask: soft limit at 83886080\n","2024-04-23 18:01:51,543 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2024-04-23 18:01:51,543 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2024-04-23 18:01:51,549 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2024-04-23 18:01:51,559 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./swap_keyval.sh]\n","2024-04-23 18:01:51,567 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n","2024-04-23 18:01:51,579 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n","2024-04-23 18:01:51,580 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n","2024-04-23 18:01:51,580 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n","2024-04-23 18:01:51,581 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n","2024-04-23 18:01:51,582 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n","2024-04-23 18:01:51,584 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n","2024-04-23 18:01:51,584 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n","2024-04-23 18:01:51,584 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n","2024-04-23 18:01:51,589 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n","2024-04-23 18:01:51,590 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n","2024-04-23 18:01:51,591 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n","2024-04-23 18:01:51,621 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2024-04-23 18:01:51,621 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2024-04-23 18:01:51,623 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2024-04-23 18:01:51,639 INFO streaming.PipeMapRed: Records R/W=746/1\n","2024-04-23 18:01:51,676 INFO streaming.PipeMapRed: MRErrorThread done\n","2024-04-23 18:01:51,680 INFO streaming.PipeMapRed: mapRedFinished\n","2024-04-23 18:01:51,686 INFO mapred.LocalJobRunner: \n","2024-04-23 18:01:51,686 INFO mapred.MapTask: Starting flush of map output\n","2024-04-23 18:01:51,686 INFO mapred.MapTask: Spilling map output\n","2024-04-23 18:01:51,686 INFO mapred.MapTask: bufstart = 0; bufend = 27907; bufvoid = 104857600\n","2024-04-23 18:01:51,686 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26211416(104845664); length = 2981/6553600\n","2024-04-23 18:01:51,704 INFO mapred.MapTask: Finished spill 0\n","2024-04-23 18:01:51,747 INFO mapred.Task: Task:attempt_local61580702_0001_m_000000_0 is done. And is in the process of committing\n","2024-04-23 18:01:51,751 INFO mapred.LocalJobRunner: Records R/W=746/1\n","2024-04-23 18:01:51,751 INFO mapred.Task: Task 'attempt_local61580702_0001_m_000000_0' done.\n","2024-04-23 18:01:51,767 INFO mapred.Task: Final Counters for attempt_local61580702_0001_m_000000_0: Counters: 17\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=30237\n","\t\tFILE: Number of bytes written=740082\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=746\n","\t\tMap output records=746\n","\t\tMap output bytes=27907\n","\t\tMap output materialized bytes=29453\n","\t\tInput split bytes=93\n","\t\tCombine input records=0\n","\t\tSpilled Records=746\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=19\n","\t\tTotal committed heap usage (bytes)=310378496\n","\tFile Input Format Counters \n","\t\tBytes Read=29596\n","2024-04-23 18:01:51,768 INFO mapred.LocalJobRunner: Finishing task: attempt_local61580702_0001_m_000000_0\n","2024-04-23 18:01:51,768 INFO mapred.LocalJobRunner: map task executor complete.\n","2024-04-23 18:01:51,782 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n","2024-04-23 18:01:51,783 INFO mapred.LocalJobRunner: Starting task: attempt_local61580702_0001_r_000000_0\n","2024-04-23 18:01:51,816 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-04-23 18:01:51,816 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-04-23 18:01:51,817 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2024-04-23 18:01:51,824 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@309c1311\n","2024-04-23 18:01:51,827 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2024-04-23 18:01:51,872 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n","2024-04-23 18:01:51,890 INFO reduce.EventFetcher: attempt_local61580702_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n","2024-04-23 18:01:51,947 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local61580702_0001_m_000000_0 decomp: 29449 len: 29453 to MEMORY\n","2024-04-23 18:01:51,954 INFO reduce.InMemoryMapOutput: Read 29449 bytes from map-output for attempt_local61580702_0001_m_000000_0\n","2024-04-23 18:01:51,960 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 29449, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->29449\n","2024-04-23 18:01:51,965 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n","2024-04-23 18:01:51,967 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-04-23 18:01:51,967 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n","2024-04-23 18:01:51,978 INFO mapred.Merger: Merging 1 sorted segments\n","2024-04-23 18:01:51,979 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 29445 bytes\n","2024-04-23 18:01:51,986 INFO reduce.MergeManagerImpl: Merged 1 segments, 29449 bytes to disk to satisfy reduce memory limit\n","2024-04-23 18:01:51,987 INFO reduce.MergeManagerImpl: Merging 1 files, 29453 bytes from disk\n","2024-04-23 18:01:51,988 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n","2024-04-23 18:01:51,989 INFO mapred.Merger: Merging 1 sorted segments\n","2024-04-23 18:01:51,990 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 29445 bytes\n","2024-04-23 18:01:51,991 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-04-23 18:01:52,027 INFO mapred.Task: Task:attempt_local61580702_0001_r_000000_0 is done. And is in the process of committing\n","2024-04-23 18:01:52,033 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-04-23 18:01:52,033 INFO mapred.Task: Task attempt_local61580702_0001_r_000000_0 is allowed to commit now\n","2024-04-23 18:01:52,036 INFO output.FileOutputCommitter: Saved output of task 'attempt_local61580702_0001_r_000000_0' to file:/content/wordcount/output2\n","2024-04-23 18:01:52,038 INFO mapred.LocalJobRunner: reduce > reduce\n","2024-04-23 18:01:52,038 INFO mapred.Task: Task 'attempt_local61580702_0001_r_000000_0' done.\n","2024-04-23 18:01:52,039 INFO mapred.Task: Final Counters for attempt_local61580702_0001_r_000000_0: Counters: 24\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=89175\n","\t\tFILE: Number of bytes written=797623\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=15\n","\t\tReduce shuffle bytes=29453\n","\t\tReduce input records=746\n","\t\tReduce output records=746\n","\t\tSpilled Records=746\n","\t\tShuffled Maps =1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=1\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=310378496\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Output Format Counters \n","\t\tBytes Written=28088\n","2024-04-23 18:01:52,040 INFO mapred.LocalJobRunner: Finishing task: attempt_local61580702_0001_r_000000_0\n","2024-04-23 18:01:52,040 INFO mapred.LocalJobRunner: reduce task executor complete.\n","2024-04-23 18:01:52,197 INFO mapreduce.Job: Job job_local61580702_0001 running in uber mode : false\n","2024-04-23 18:01:52,198 INFO mapreduce.Job:  map 100% reduce 100%\n","2024-04-23 18:01:52,200 INFO mapreduce.Job: Job job_local61580702_0001 completed successfully\n","2024-04-23 18:01:52,214 INFO mapreduce.Job: Counters: 30\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=119412\n","\t\tFILE: Number of bytes written=1537705\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=746\n","\t\tMap output records=746\n","\t\tMap output bytes=27907\n","\t\tMap output materialized bytes=29453\n","\t\tInput split bytes=93\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=15\n","\t\tReduce shuffle bytes=29453\n","\t\tReduce input records=746\n","\t\tReduce output records=746\n","\t\tSpilled Records=1492\n","\t\tShuffled Maps =1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=1\n","\t\tGC time elapsed (ms)=19\n","\t\tTotal committed heap usage (bytes)=620756992\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Input Format Counters \n","\t\tBytes Read=29596\n","\tFile Output Format Counters \n","\t\tBytes Written=28088\n","2024-04-23 18:01:52,214 INFO streaming.StreamJob: Output directory: wordcount/output2\n"]}],"source":["%%bash\n","hdfs dfs -rm -r wordcount/output2 2>/dev/null\n","mapred streaming \\\n","  -file swap_keyval.sh \\\n","  -input wordcount/output \\\n","  -output wordcount/output2 \\\n","  -mapper swap_keyval.sh"]},{"cell_type":"markdown","metadata":{"id":"KGa-qTjkD3Fd"},"source":["Check the output on HDFS"]},{"cell_type":"code","execution_count":58,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IDj-FrWZD3Fd","executionInfo":{"status":"ok","timestamp":1713895313082,"user_tz":-420,"elapsed":1442,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}},"outputId":"edf98dad-6731-45a7-da18-cb41afa5d1a9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 2 items\n","-rw-r--r--   1 root root          0 2024-04-23 18:01 wordcount/output2/_SUCCESS\n","-rw-r--r--   1 root root      27860 2024-04-23 18:01 wordcount/output2/part-00000\n"]}],"source":["!hdfs dfs -ls wordcount/output2"]},{"cell_type":"code","execution_count":59,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OZs75WfwD3Fd","executionInfo":{"status":"ok","timestamp":1713895316173,"user_tz":-420,"elapsed":3093,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}},"outputId":"e8459907-e051-49f8-e60f-290f15b5ae71"},"outputs":[{"output_type":"stream","name":"stdout","text":["1\t!!n.frames[t];\n","1\tüberraschen.\n","1\tüber\n","1\t©\n","1\t},\n","1\t}();\n","1\t}(),\n","1\t{};\n","1\ty(){E[\"default\"].debug(\"User\n","1\ty(),j(),void(ne=D());case\n","cat: Unable to write to output stream.\n"]}],"source":["!hdfs dfs -cat wordcount/output2/part-00000|head"]},{"cell_type":"markdown","metadata":{"id":"mwU4MGGVD3Fe"},"source":["Mapper uses by default ascending order to sort by key. We could have changed that with an option but for now let's look at the end of the file."]},{"cell_type":"code","execution_count":60,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"COXboJaHD3Fe","executionInfo":{"status":"ok","timestamp":1713895319144,"user_tz":-420,"elapsed":2975,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}},"outputId":"2a9058a4-9868-4929-f2ea-7ab561ad7552"},"outputs":[{"output_type":"stream","name":"stdout","text":["7\t==\n","7\t:\n","7\tdie\n","7\t?\n","7\tif\n","7\ttypeof\n","8\t0\n","8\tn(e){return\n","9\te&&e.__esModule?e:{\"default\":e}}function\n","9\tr\n"]}],"source":["!hdfs dfs -cat wordcount/output2/part-00000|tail"]},{"cell_type":"markdown","metadata":{"id":"1QRPxpgtD3Fe"},"source":["### Configure sort with `KeyFieldBasedComparator` <a name=\"KeyFieldBasedComparator\"></a>\n","\n","In general, we can determine how mappers are going to sort their output by configuring the comparator directive to use the special class [`KeyFieldBasedComparator`](https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapreduce/lib/partition/KeyFieldBasedComparator.html)\n","<html><pre>-D mapreduce.job.output.key.comparator.class=\\\n","    org.apache.hadoop.mapred.lib.KeyFieldBasedComparator</pre></html>\n","    \n","This class has some options similar to the Unix `sort`(`-n` to sort numerically, `-r` for reverse sorting, `-k pos1[,pos2]` for specifying fields to sort by).\n","\n","Let us see the comparator in action on our data to get the desired result. Note that this time we are removing `output2` because we're running the second mapreduce job again."]},{"cell_type":"code","execution_count":61,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yckLOf7dD3Fe","executionInfo":{"status":"ok","timestamp":1713895326355,"user_tz":-420,"elapsed":7215,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}},"outputId":"2acd66f3-948d-47b5-f772-b9cf13abc2df"},"outputs":[{"output_type":"stream","name":"stdout","text":["Deleted wordcount/output2\n","packageJobJar: [swap_keyval.sh] [] /tmp/streamjob3962628410173791922.jar tmpDir=null\n"]},{"output_type":"stream","name":"stderr","text":["2024-04-23 18:02:03,909 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n","2024-04-23 18:02:04,812 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n","2024-04-23 18:02:04,992 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n","2024-04-23 18:02:04,992 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n","2024-04-23 18:02:05,024 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2024-04-23 18:02:05,333 INFO mapred.FileInputFormat: Total input files to process : 1\n","2024-04-23 18:02:05,362 INFO mapreduce.JobSubmitter: number of splits:1\n","2024-04-23 18:02:05,739 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local848978334_0001\n","2024-04-23 18:02:05,739 INFO mapreduce.JobSubmitter: Executing with tokens: []\n","2024-04-23 18:02:06,279 INFO mapred.LocalDistributedCacheManager: Localized file:/content/swap_keyval.sh as file:/tmp/hadoop-root/mapred/local/job_local848978334_0001_ea3b0e1a-e46b-44c2-9aea-a05067d25938/swap_keyval.sh\n","2024-04-23 18:02:06,431 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n","2024-04-23 18:02:06,434 INFO mapreduce.Job: Running job: job_local848978334_0001\n","2024-04-23 18:02:06,443 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n","2024-04-23 18:02:06,446 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n","2024-04-23 18:02:06,456 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-04-23 18:02:06,456 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-04-23 18:02:06,532 INFO mapred.LocalJobRunner: Waiting for map tasks\n","2024-04-23 18:02:06,540 INFO mapred.LocalJobRunner: Starting task: attempt_local848978334_0001_m_000000_0\n","2024-04-23 18:02:06,589 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-04-23 18:02:06,591 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-04-23 18:02:06,634 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2024-04-23 18:02:06,648 INFO mapred.MapTask: Processing split: file:/content/wordcount/output/part-00000:0+29352\n","2024-04-23 18:02:06,680 INFO mapred.MapTask: numReduceTasks: 1\n","2024-04-23 18:02:06,798 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2024-04-23 18:02:06,798 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2024-04-23 18:02:06,798 INFO mapred.MapTask: soft limit at 83886080\n","2024-04-23 18:02:06,799 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2024-04-23 18:02:06,799 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2024-04-23 18:02:06,814 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2024-04-23 18:02:06,829 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./swap_keyval.sh]\n","2024-04-23 18:02:06,837 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n","2024-04-23 18:02:06,838 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n","2024-04-23 18:02:06,841 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n","2024-04-23 18:02:06,841 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n","2024-04-23 18:02:06,842 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n","2024-04-23 18:02:06,843 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n","2024-04-23 18:02:06,845 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n","2024-04-23 18:02:06,845 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n","2024-04-23 18:02:06,845 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n","2024-04-23 18:02:06,846 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n","2024-04-23 18:02:06,847 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n","2024-04-23 18:02:06,848 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n","2024-04-23 18:02:06,876 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2024-04-23 18:02:06,876 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2024-04-23 18:02:06,878 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2024-04-23 18:02:06,904 INFO streaming.PipeMapRed: Records R/W=746/1\n","2024-04-23 18:02:06,948 INFO streaming.PipeMapRed: MRErrorThread done\n","2024-04-23 18:02:06,949 INFO streaming.PipeMapRed: mapRedFinished\n","2024-04-23 18:02:06,954 INFO mapred.LocalJobRunner: \n","2024-04-23 18:02:06,954 INFO mapred.MapTask: Starting flush of map output\n","2024-04-23 18:02:06,954 INFO mapred.MapTask: Spilling map output\n","2024-04-23 18:02:06,954 INFO mapred.MapTask: bufstart = 0; bufend = 27907; bufvoid = 104857600\n","2024-04-23 18:02:06,955 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26211416(104845664); length = 2981/6553600\n","2024-04-23 18:02:06,985 INFO mapred.MapTask: Finished spill 0\n","2024-04-23 18:02:07,023 INFO mapred.Task: Task:attempt_local848978334_0001_m_000000_0 is done. And is in the process of committing\n","2024-04-23 18:02:07,027 INFO mapred.LocalJobRunner: Records R/W=746/1\n","2024-04-23 18:02:07,028 INFO mapred.Task: Task 'attempt_local848978334_0001_m_000000_0' done.\n","2024-04-23 18:02:07,043 INFO mapred.Task: Final Counters for attempt_local848978334_0001_m_000000_0: Counters: 17\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=30237\n","\t\tFILE: Number of bytes written=744453\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=746\n","\t\tMap output records=746\n","\t\tMap output bytes=27907\n","\t\tMap output materialized bytes=29453\n","\t\tInput split bytes=93\n","\t\tCombine input records=0\n","\t\tSpilled Records=746\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=15\n","\t\tTotal committed heap usage (bytes)=432013312\n","\tFile Input Format Counters \n","\t\tBytes Read=29596\n","2024-04-23 18:02:07,043 INFO mapred.LocalJobRunner: Finishing task: attempt_local848978334_0001_m_000000_0\n","2024-04-23 18:02:07,044 INFO mapred.LocalJobRunner: map task executor complete.\n","2024-04-23 18:02:07,058 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n","2024-04-23 18:02:07,072 INFO mapred.LocalJobRunner: Starting task: attempt_local848978334_0001_r_000000_0\n","2024-04-23 18:02:07,096 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-04-23 18:02:07,096 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-04-23 18:02:07,096 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2024-04-23 18:02:07,101 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@394638a4\n","2024-04-23 18:02:07,105 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2024-04-23 18:02:07,155 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n","2024-04-23 18:02:07,167 INFO reduce.EventFetcher: attempt_local848978334_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n","2024-04-23 18:02:07,231 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local848978334_0001_m_000000_0 decomp: 29449 len: 29453 to MEMORY\n","2024-04-23 18:02:07,238 INFO reduce.InMemoryMapOutput: Read 29449 bytes from map-output for attempt_local848978334_0001_m_000000_0\n","2024-04-23 18:02:07,245 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 29449, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->29449\n","2024-04-23 18:02:07,250 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n","2024-04-23 18:02:07,252 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-04-23 18:02:07,254 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n","2024-04-23 18:02:07,266 INFO mapred.Merger: Merging 1 sorted segments\n","2024-04-23 18:02:07,267 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 29444 bytes\n","2024-04-23 18:02:07,291 INFO reduce.MergeManagerImpl: Merged 1 segments, 29449 bytes to disk to satisfy reduce memory limit\n","2024-04-23 18:02:07,292 INFO reduce.MergeManagerImpl: Merging 1 files, 29453 bytes from disk\n","2024-04-23 18:02:07,294 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n","2024-04-23 18:02:07,294 INFO mapred.Merger: Merging 1 sorted segments\n","2024-04-23 18:02:07,297 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 29444 bytes\n","2024-04-23 18:02:07,299 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-04-23 18:02:07,346 INFO mapred.Task: Task:attempt_local848978334_0001_r_000000_0 is done. And is in the process of committing\n","2024-04-23 18:02:07,350 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-04-23 18:02:07,351 INFO mapred.Task: Task attempt_local848978334_0001_r_000000_0 is allowed to commit now\n","2024-04-23 18:02:07,356 INFO output.FileOutputCommitter: Saved output of task 'attempt_local848978334_0001_r_000000_0' to file:/content/wordcount/output2\n","2024-04-23 18:02:07,360 INFO mapred.LocalJobRunner: reduce > reduce\n","2024-04-23 18:02:07,361 INFO mapred.Task: Task 'attempt_local848978334_0001_r_000000_0' done.\n","2024-04-23 18:02:07,363 INFO mapred.Task: Final Counters for attempt_local848978334_0001_r_000000_0: Counters: 24\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=89175\n","\t\tFILE: Number of bytes written=801994\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=15\n","\t\tReduce shuffle bytes=29453\n","\t\tReduce input records=746\n","\t\tReduce output records=746\n","\t\tSpilled Records=746\n","\t\tShuffled Maps =1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=1\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=432013312\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Output Format Counters \n","\t\tBytes Written=28088\n","2024-04-23 18:02:07,363 INFO mapred.LocalJobRunner: Finishing task: attempt_local848978334_0001_r_000000_0\n","2024-04-23 18:02:07,363 INFO mapred.LocalJobRunner: reduce task executor complete.\n","2024-04-23 18:02:07,441 INFO mapreduce.Job: Job job_local848978334_0001 running in uber mode : false\n","2024-04-23 18:02:07,442 INFO mapreduce.Job:  map 100% reduce 100%\n","2024-04-23 18:02:07,444 INFO mapreduce.Job: Job job_local848978334_0001 completed successfully\n","2024-04-23 18:02:07,455 INFO mapreduce.Job: Counters: 30\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=119412\n","\t\tFILE: Number of bytes written=1546447\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=746\n","\t\tMap output records=746\n","\t\tMap output bytes=27907\n","\t\tMap output materialized bytes=29453\n","\t\tInput split bytes=93\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=15\n","\t\tReduce shuffle bytes=29453\n","\t\tReduce input records=746\n","\t\tReduce output records=746\n","\t\tSpilled Records=1492\n","\t\tShuffled Maps =1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=1\n","\t\tGC time elapsed (ms)=15\n","\t\tTotal committed heap usage (bytes)=864026624\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Input Format Counters \n","\t\tBytes Read=29596\n","\tFile Output Format Counters \n","\t\tBytes Written=28088\n","2024-04-23 18:02:07,455 INFO streaming.StreamJob: Output directory: wordcount/output2\n"]}],"source":["%%bash\n","hdfs dfs -rmr wordcount/output2 2>/dev/null\n","comparator_class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator\n","mapred streaming \\\n","  -D mapreduce.job.output.key.comparator.class=$comparator_class \\\n","  -D mapreduce.partition.keycomparator.options=-nr \\\n","  -file swap_keyval.sh \\\n","  -input wordcount/output \\\n","  -output wordcount/output2 \\\n","  -mapper swap_keyval.sh"]},{"cell_type":"code","execution_count":62,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yh2cIb9mD3Fe","executionInfo":{"status":"ok","timestamp":1713895328587,"user_tz":-420,"elapsed":2235,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}},"outputId":"388e8b64-b60d-4bfe-82b0-106eca5547e9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 2 items\n","-rw-r--r--   1 root root          0 2024-04-23 18:02 wordcount/output2/_SUCCESS\n","-rw-r--r--   1 root root      27860 2024-04-23 18:02 wordcount/output2/part-00000\n"]}],"source":["!hdfs dfs -ls wordcount/output2"]},{"cell_type":"code","execution_count":63,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eWxjdQ7hD3Fe","executionInfo":{"status":"ok","timestamp":1713895331681,"user_tz":-420,"elapsed":3097,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}},"outputId":"d6d00972-61b5-4ad7-ca1c-5c236a5f8830"},"outputs":[{"output_type":"stream","name":"stdout","text":["40\t=\n","22\t{\n","22\tvar\n","19\t&&\n","13\tstrict\";function\n","12\tnot\n","12\t}\n","12\tin\n","10\tto\n","9\te&&e.__esModule?e:{\"default\":e}}function\n","cat: Unable to write to output stream.\n"]}],"source":["!hdfs dfs -cat wordcount/output2/part-00000|head"]},{"cell_type":"markdown","metadata":{"id":"APPySwzzD3Ff"},"source":["Now we get the output in the desired order."]},{"cell_type":"markdown","metadata":{"id":"alzQgJAnD3Fi"},"source":["### Specifying Configuration Variables with the -D Option <a name=\"configuration_variables\"></a>\n","\n","With the `-D` option it is possible to override options set in the default configuration file [`mapred_default.xml`](https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml)\n","(see the [Apache Hadoop documentation](https://hadoop.apache.org/docs/current/hadoop-streaming/HadoopStreaming.html#Specifying_Configuration_Variables_with_the_-D_Option)).\n","\n","One option that might come handy when dealing with out-of-memory issues in the sorting phase is the size in MB of the memory reserved for sorting `mapreduce.task.io.sort.mb`:\n"," <html>\n","    <pre>-D mapreduce.task.io.sort.mb=512\n","    </pre>\n"," </html>\n","\n"," **Note:** the maximum value for `mapreduce.task.io.sort.mb` is 2047.   "]},{"cell_type":"markdown","metadata":{"id":"9wYKD9aID3Fi"},"source":["## What is word count useful for? <a name=\"wordcount\"></a>\n","Counting the frequencies of words is at the basis of _indexing_ and it facilitates the retrieval of relevant documents in search engines."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}