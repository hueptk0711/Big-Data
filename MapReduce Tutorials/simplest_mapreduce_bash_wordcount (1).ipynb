{"cells":[{"cell_type":"markdown","metadata":{"id":"4iIw-8oyIy-f"},"source":["# Simple distributed wordcount with MapReduce"]},{"cell_type":"markdown","metadata":{"id":"wT6lnV46Iy-g"},"source":["Check that file `file.txt` exists, view size."]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6vyHKMVBIy-g","executionInfo":{"status":"ok","timestamp":1713895660291,"user_tz":-420,"elapsed":605,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}},"outputId":"cb736e88-5460-4629-ed78-6c9c4f42ee09"},"outputs":[{"output_type":"stream","name":"stdout","text":["-rw-r--r-- 1 root root 58K Apr 23 18:06 file.txt\n"]}],"source":["!ls -hal file.txt"]},{"cell_type":"code","source":["HADOOP_URL = \"https://dlcdn.apache.org/hadoop/common/stable/hadoop-3.4.0.tar.gz\"\n","\n","import requests\n","import os\n","import tarfile\n","\n","def download_and_extract_targz(url):\n","    response = requests.get(url)\n","    filename = url.rsplit('/', 1)[-1]\n","    HADOOP_HOME = filename[:-7]\n","    # set HADOOP_HOME environment variable\n","    os.environ['HADOOP_HOME'] = HADOOP_HOME\n","    if os.path.isdir(HADOOP_HOME):\n","      print(\"Not downloading, Hadoop folder {} already exists\".format(HADOOP_HOME))\n","      return\n","    if response.status_code == 200:\n","        with open(filename, 'wb') as file:\n","            file.write(response.content)\n","        with tarfile.open(filename, 'r:gz') as tar_ref:\n","            extract_path = tar_ref.extractall(path='.')\n","            # Get the names of all members (files and directories) in the archive\n","            all_members = tar_ref.getnames()\n","            # If there is a top-level directory, get its name\n","            if all_members:\n","              top_level_directory = all_members[0]\n","              print(f\"ZIP file downloaded and extracted successfully. Contents saved at: {top_level_directory}\")\n","    else:\n","        print(f\"Failed to download ZIP file. Status code: {response.status_code}\")\n","\n","\n","download_and_extract_targz(HADOOP_URL)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iVxW55wJI-Js","executionInfo":{"status":"ok","timestamp":1713895726718,"user_tz":-420,"elapsed":65932,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}},"outputId":"8b23112d-12de-40cf-b04a-f7f2a5798b2e"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["ZIP file downloaded and extracted successfully. Contents saved at: hadoop-3.4.0\n"]}]},{"cell_type":"code","source":["\n","# HADOOP_HOME was set earlier when downloading Hadoop distribution\n","print(\"HADOOP_HOME is {}\".format(os.environ['HADOOP_HOME']))\n","\n","os.environ['PATH'] = ':'.join([os.path.join(os.environ['HADOOP_HOME'], 'bin'), os.environ['PATH']])\n","print(\"PATH is {}\".format(os.environ['PATH']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y_LrmNl2JBbr","executionInfo":{"status":"ok","timestamp":1713895726719,"user_tz":-420,"elapsed":14,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}},"outputId":"f4c612dc-b13e-4c52-fd31-cb54a6bb9624"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["HADOOP_HOME is hadoop-3.4.0\n","PATH is hadoop-3.4.0/bin:/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"]}]},{"cell_type":"code","source":["import shutil\n","\n","# set variable JAVA_HOME (install Java if necessary)\n","def is_java_installed():\n","    os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n","    return os.environ['JAVA_HOME']\n","\n","def install_java():\n","    # Uncomment and modify the desired version\n","    # java_version= 'openjdk-11-jre-headless'\n","    # java_version= 'default-jre'\n","    # java_version= 'openjdk-17-jre-headless'\n","    # java_version= 'openjdk-18-jre-headless'\n","    java_version= 'openjdk-19-jre-headless'\n","\n","    print(f\"Java not found. Installing {java_version} ... (this might take a while)\")\n","    try:\n","        cmd = f\"apt install -y {java_version}\"\n","        subprocess_output = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n","        stdout_result = subprocess_output.stdout\n","        # Process the results as needed\n","        print(\"Done installing Java {}\".format(java_version))\n","        os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n","        print(\"JAVA_HOME is {}\".format(os.environ['JAVA_HOME']))\n","    except subprocess.CalledProcessError as e:\n","        # Handle the error if the command returns a non-zero exit code\n","        print(\"Command failed with return code {}\".format(e.returncode))\n","        print(\"stdout: {}\".format(e.stdout))\n","\n","# Install Java if not available\n","if is_java_installed():\n","    print(\"Java is already installed: {}\".format(os.environ['JAVA_HOME']))\n","else:\n","    print(\"Installing Java\")\n","    install_java()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HyDtjvG-JDTZ","executionInfo":{"status":"ok","timestamp":1713895726719,"user_tz":-420,"elapsed":10,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}},"outputId":"f4734bbd-0bd7-42b0-81ff-64641acb5ae6"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Java is already installed: /usr/lib/jvm/java-11-openjdk-amd64\n"]}]},{"cell_type":"markdown","metadata":{"id":"a73JxYdnIy-g"},"source":["Copy file to HDFS"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"Nt9PehYYIy-h","executionInfo":{"status":"ok","timestamp":1713895730477,"user_tz":-420,"elapsed":3764,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}}},"outputs":[],"source":["!hdfs dfs -put -f file.txt"]},{"cell_type":"markdown","metadata":{"id":"YtztjvZrIy-h"},"source":["Erase `result` folder."]},{"cell_type":"code","execution_count":18,"metadata":{"id":"tpdbbm1MIy-h","executionInfo":{"status":"ok","timestamp":1713895733541,"user_tz":-420,"elapsed":3095,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}}},"outputs":[],"source":["!hdfs dfs -rm -R result 2>/dev/null"]},{"cell_type":"markdown","metadata":{"id":"z9rqcZQrIy-h"},"source":["Run the bash wordcount command `wc` in parallel on the distributed file."]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"99_t2pSKIy-h","executionInfo":{"status":"ok","timestamp":1713895739145,"user_tz":-420,"elapsed":5611,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}},"outputId":"c18f85d7-b25f-4269-bacf-6e2cf19ad1fb"},"outputs":[{"output_type":"stream","name":"stdout","text":["2024-04-23 18:08:57,267 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n","2024-04-23 18:08:57,584 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n","2024-04-23 18:08:57,584 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n","2024-04-23 18:08:57,616 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2024-04-23 18:08:58,095 INFO mapred.FileInputFormat: Total input files to process : 1\n","2024-04-23 18:08:58,141 INFO mapreduce.JobSubmitter: number of splits:1\n","2024-04-23 18:08:58,708 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local780858888_0001\n","2024-04-23 18:08:58,708 INFO mapreduce.JobSubmitter: Executing with tokens: []\n","2024-04-23 18:08:59,079 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n","2024-04-23 18:08:59,081 INFO mapreduce.Job: Running job: job_local780858888_0001\n","2024-04-23 18:08:59,104 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n","2024-04-23 18:08:59,112 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n","2024-04-23 18:08:59,126 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-04-23 18:08:59,126 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-04-23 18:08:59,202 INFO mapred.LocalJobRunner: Waiting for map tasks\n","2024-04-23 18:08:59,213 INFO mapred.LocalJobRunner: Starting task: attempt_local780858888_0001_m_000000_0\n","2024-04-23 18:08:59,258 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-04-23 18:08:59,261 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-04-23 18:08:59,306 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2024-04-23 18:08:59,317 INFO mapred.MapTask: Processing split: file:/content/file.txt:0+58911\n","2024-04-23 18:08:59,338 INFO mapred.MapTask: numReduceTasks: 1\n","2024-04-23 18:08:59,477 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2024-04-23 18:08:59,477 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2024-04-23 18:08:59,477 INFO mapred.MapTask: soft limit at 83886080\n","2024-04-23 18:08:59,477 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2024-04-23 18:08:59,477 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2024-04-23 18:08:59,485 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2024-04-23 18:08:59,489 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n","2024-04-23 18:08:59,501 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n","2024-04-23 18:08:59,503 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n","2024-04-23 18:08:59,503 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n","2024-04-23 18:08:59,504 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n","2024-04-23 18:08:59,505 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n","2024-04-23 18:08:59,505 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n","2024-04-23 18:08:59,508 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n","2024-04-23 18:08:59,508 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n","2024-04-23 18:08:59,509 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n","2024-04-23 18:08:59,509 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n","2024-04-23 18:08:59,510 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n","2024-04-23 18:08:59,511 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n","2024-04-23 18:08:59,556 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2024-04-23 18:08:59,557 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2024-04-23 18:08:59,560 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2024-04-23 18:08:59,572 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2024-04-23 18:08:59,576 INFO streaming.PipeMapRed: Records R/W=1180/1\n","2024-04-23 18:08:59,591 INFO streaming.PipeMapRed: MRErrorThread done\n","2024-04-23 18:08:59,613 INFO streaming.PipeMapRed: mapRedFinished\n","2024-04-23 18:08:59,623 INFO mapred.LocalJobRunner: \n","2024-04-23 18:08:59,623 INFO mapred.MapTask: Starting flush of map output\n","2024-04-23 18:08:59,623 INFO mapred.MapTask: Spilling map output\n","2024-04-23 18:08:59,623 INFO mapred.MapTask: bufstart = 0; bufend = 58913; bufvoid = 104857600\n","2024-04-23 18:08:59,623 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26209680(104838720); length = 4717/6553600\n","2024-04-23 18:08:59,657 INFO mapred.MapTask: Finished spill 0\n","2024-04-23 18:08:59,691 INFO mapred.Task: Task:attempt_local780858888_0001_m_000000_0 is done. And is in the process of committing\n","2024-04-23 18:08:59,696 INFO mapred.LocalJobRunner: Records R/W=1180/1\n","2024-04-23 18:08:59,697 INFO mapred.Task: Task 'attempt_local780858888_0001_m_000000_0' done.\n","2024-04-23 18:08:59,707 INFO mapred.Task: Final Counters for attempt_local780858888_0001_m_000000_0: Counters: 17\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=201292\n","\t\tFILE: Number of bytes written=917112\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1180\n","\t\tMap output records=1180\n","\t\tMap output bytes=58913\n","\t\tMap output materialized bytes=61279\n","\t\tInput split bytes=74\n","\t\tCombine input records=0\n","\t\tSpilled Records=1180\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=371195904\n","\tFile Input Format Counters \n","\t\tBytes Read=59391\n","2024-04-23 18:08:59,707 INFO mapred.LocalJobRunner: Finishing task: attempt_local780858888_0001_m_000000_0\n","2024-04-23 18:08:59,708 INFO mapred.LocalJobRunner: map task executor complete.\n","2024-04-23 18:08:59,715 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n","2024-04-23 18:08:59,719 INFO mapred.LocalJobRunner: Starting task: attempt_local780858888_0001_r_000000_0\n","2024-04-23 18:08:59,737 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-04-23 18:08:59,737 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-04-23 18:08:59,739 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2024-04-23 18:08:59,754 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@134834ca\n","2024-04-23 18:08:59,767 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2024-04-23 18:08:59,827 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n","2024-04-23 18:08:59,844 INFO reduce.EventFetcher: attempt_local780858888_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n","2024-04-23 18:08:59,916 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local780858888_0001_m_000000_0 decomp: 61275 len: 61279 to MEMORY\n","2024-04-23 18:08:59,926 INFO reduce.InMemoryMapOutput: Read 61275 bytes from map-output for attempt_local780858888_0001_m_000000_0\n","2024-04-23 18:08:59,932 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 61275, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->61275\n","2024-04-23 18:08:59,937 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n","2024-04-23 18:08:59,939 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-04-23 18:08:59,939 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n","2024-04-23 18:08:59,949 INFO mapred.Merger: Merging 1 sorted segments\n","2024-04-23 18:08:59,949 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 61272 bytes\n","2024-04-23 18:08:59,963 INFO reduce.MergeManagerImpl: Merged 1 segments, 61275 bytes to disk to satisfy reduce memory limit\n","2024-04-23 18:08:59,964 INFO reduce.MergeManagerImpl: Merging 1 files, 61279 bytes from disk\n","2024-04-23 18:08:59,965 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n","2024-04-23 18:08:59,965 INFO mapred.Merger: Merging 1 sorted segments\n","2024-04-23 18:08:59,968 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 61272 bytes\n","2024-04-23 18:08:59,969 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-04-23 18:08:59,971 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/bin/wc]\n","2024-04-23 18:08:59,976 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n","2024-04-23 18:08:59,979 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n","2024-04-23 18:09:00,001 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2024-04-23 18:09:00,002 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n","2024-04-23 18:09:00,004 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n","2024-04-23 18:09:00,019 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n","2024-04-23 18:09:00,029 INFO streaming.PipeMapRed: MRErrorThread done\n","2024-04-23 18:09:00,030 INFO streaming.PipeMapRed: Records R/W=1180/1\n","2024-04-23 18:09:00,032 INFO streaming.PipeMapRed: mapRedFinished\n","2024-04-23 18:09:00,034 INFO mapred.Task: Task:attempt_local780858888_0001_r_000000_0 is done. And is in the process of committing\n","2024-04-23 18:09:00,036 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-04-23 18:09:00,036 INFO mapred.Task: Task attempt_local780858888_0001_r_000000_0 is allowed to commit now\n","2024-04-23 18:09:00,038 INFO output.FileOutputCommitter: Saved output of task 'attempt_local780858888_0001_r_000000_0' to file:/content/result\n","2024-04-23 18:09:00,039 INFO mapred.LocalJobRunner: Records R/W=1180/1 > reduce\n","2024-04-23 18:09:00,040 INFO mapred.Task: Task 'attempt_local780858888_0001_r_000000_0' done.\n","2024-04-23 18:09:00,040 INFO mapred.Task: Final Counters for attempt_local780858888_0001_r_000000_0: Counters: 24\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=323882\n","\t\tFILE: Number of bytes written=978428\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=919\n","\t\tReduce shuffle bytes=61279\n","\t\tReduce input records=1180\n","\t\tReduce output records=1\n","\t\tSpilled Records=1180\n","\t\tShuffled Maps =1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=1\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=371195904\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Output Format Counters \n","\t\tBytes Written=37\n","2024-04-23 18:09:00,041 INFO mapred.LocalJobRunner: Finishing task: attempt_local780858888_0001_r_000000_0\n","2024-04-23 18:09:00,041 INFO mapred.LocalJobRunner: reduce task executor complete.\n","2024-04-23 18:09:00,104 INFO mapreduce.Job: Job job_local780858888_0001 running in uber mode : false\n","2024-04-23 18:09:00,105 INFO mapreduce.Job:  map 100% reduce 100%\n","2024-04-23 18:09:00,107 INFO mapreduce.Job: Job job_local780858888_0001 completed successfully\n","2024-04-23 18:09:00,136 INFO mapreduce.Job: Counters: 30\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=525174\n","\t\tFILE: Number of bytes written=1895540\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1180\n","\t\tMap output records=1180\n","\t\tMap output bytes=58913\n","\t\tMap output materialized bytes=61279\n","\t\tInput split bytes=74\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=919\n","\t\tReduce shuffle bytes=61279\n","\t\tReduce input records=1180\n","\t\tReduce output records=1\n","\t\tSpilled Records=2360\n","\t\tShuffled Maps =1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=1\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=742391808\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Input Format Counters \n","\t\tBytes Read=59391\n","\tFile Output Format Counters \n","\t\tBytes Written=37\n","2024-04-23 18:09:00,136 INFO streaming.StreamJob: Output directory: result\n"]}],"source":["!mapred streaming \\\n","  -input file.txt \\\n","  -output result \\\n","  -mapper /bin/cat \\\n","  -reducer /usr/bin/wc"]},{"cell_type":"markdown","metadata":{"id":"nTPqmBGeIy-i"},"source":["Check result of MapReduce job"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WmquUffXIy-i","executionInfo":{"status":"ok","timestamp":1713895742183,"user_tz":-420,"elapsed":3043,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}},"outputId":"98779b85-f607-4378-b99b-f3fafffab9f5"},"outputs":[{"output_type":"stream","name":"stdout","text":["   1180   10391   58913\t\n"]}],"source":["!hdfs dfs -cat result/part*"]},{"cell_type":"markdown","metadata":{"id":"G5kCAxd7Iy-i"},"source":["Check that the word count is correct by comparing with `wc` on local host (warning: do not try with too large files)."]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lF-pVICoIy-i","executionInfo":{"status":"ok","timestamp":1713895742185,"user_tz":-420,"elapsed":13,"user":{"displayName":"Huệ Phạm Thị Kim","userId":"13314167880737377603"}},"outputId":"9f336391-ada0-4d4b-f725-e175c55d0acb"},"outputs":[{"output_type":"stream","name":"stdout","text":[" 1179 10391 58911 file.txt\n"]}],"source":["!wc file.txt"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}